{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/envs/py2env/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "PROJECT = 'qwiklabs-gcp-aebfb78fe0f1b1d1' # REPLACE WITH YOUR PROJECT ID\n",
    "BUCKET = 'sarah-bucket' # REPLACE WITH YOUR BUCKET NAME\n",
    "REGION = 'us-west1-a' # REPLACE WITH YOUR BUCKET REGION e.g. us-central1\n",
    "os.environ['TFVERSION'] = '1.8'  # Tensorflow version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for bash\n",
    "os.environ['PROJECT'] = PROJECT\n",
    "os.environ['BUCKET'] = BUCKET\n",
    "os.environ['REGION'] = REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n",
      "Updated property [compute/region].\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gcloud config set project $PROJECT\n",
    "gcloud config set compute/region $REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/datalab/energy_forcasing/Sarah\n",
      "BIG_q.ipynb\t\tCNN_W2P_model  Energy_ts\n",
      "Build_model_task.ipynb\tdata\t       sarah_data.csv\n"
     ]
    }
   ],
   "source": [
    "! pwd\n",
    "! ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CommandException: No URLs matched: /content/datalab/energy_forcasing/sarah_data.csv\r\n"
     ]
    }
   ],
   "source": [
    "## move the csv file to the bucket ~ you rly dont need to do this\n",
    "!gsutil cp -p /content/datalab/energy_forcasing/Sarah/sarah_data.csv gs://sarah-bucket/sarah_data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wind_speed_100m</th>\n",
       "      <th>wind_direction_100m</th>\n",
       "      <th>temperature</th>\n",
       "      <th>air_density</th>\n",
       "      <th>pressure</th>\n",
       "      <th>precipitation</th>\n",
       "      <th>wind_gust</th>\n",
       "      <th>radiation</th>\n",
       "      <th>wind_speed</th>\n",
       "      <th>wind_direction</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>6646.000000</td>\n",
       "      <td>6646.000000</td>\n",
       "      <td>6646.000000</td>\n",
       "      <td>6646.000000</td>\n",
       "      <td>6646.000000</td>\n",
       "      <td>6646.000000</td>\n",
       "      <td>6646.000000</td>\n",
       "      <td>6646.000000</td>\n",
       "      <td>6646.000000</td>\n",
       "      <td>6646.000000</td>\n",
       "      <td>6646.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.799901</td>\n",
       "      <td>189.582975</td>\n",
       "      <td>14.920355</td>\n",
       "      <td>1.132338</td>\n",
       "      <td>935.286695</td>\n",
       "      <td>0.117364</td>\n",
       "      <td>4.872509</td>\n",
       "      <td>225.250182</td>\n",
       "      <td>3.220986</td>\n",
       "      <td>189.082629</td>\n",
       "      <td>50.153506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.964154</td>\n",
       "      <td>58.126639</td>\n",
       "      <td>9.626527</td>\n",
       "      <td>0.039333</td>\n",
       "      <td>6.105083</td>\n",
       "      <td>0.245561</td>\n",
       "      <td>2.428587</td>\n",
       "      <td>266.299815</td>\n",
       "      <td>1.319704</td>\n",
       "      <td>56.712796</td>\n",
       "      <td>12.448392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.372222</td>\n",
       "      <td>47.883333</td>\n",
       "      <td>-6.044444</td>\n",
       "      <td>1.036111</td>\n",
       "      <td>913.800000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.194444</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.855556</td>\n",
       "      <td>50.538889</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.534722</td>\n",
       "      <td>142.598611</td>\n",
       "      <td>7.540278</td>\n",
       "      <td>1.106111</td>\n",
       "      <td>932.034722</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.284722</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.294444</td>\n",
       "      <td>143.750000</td>\n",
       "      <td>43.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4.477778</td>\n",
       "      <td>191.908333</td>\n",
       "      <td>14.213889</td>\n",
       "      <td>1.131667</td>\n",
       "      <td>934.761111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.327778</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>2.961111</td>\n",
       "      <td>191.208333</td>\n",
       "      <td>50.890000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5.544444</td>\n",
       "      <td>232.043056</td>\n",
       "      <td>21.062500</td>\n",
       "      <td>1.160556</td>\n",
       "      <td>937.805556</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>5.627778</td>\n",
       "      <td>416.422222</td>\n",
       "      <td>3.850000</td>\n",
       "      <td>230.865278</td>\n",
       "      <td>59.950000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>15.044444</td>\n",
       "      <td>329.544444</td>\n",
       "      <td>41.044444</td>\n",
       "      <td>1.229444</td>\n",
       "      <td>961.238889</td>\n",
       "      <td>1.700000</td>\n",
       "      <td>16.705556</td>\n",
       "      <td>954.288889</td>\n",
       "      <td>9.738889</td>\n",
       "      <td>327.711111</td>\n",
       "      <td>85.050000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       wind_speed_100m  wind_direction_100m  temperature  air_density  \\\n",
       "count      6646.000000          6646.000000  6646.000000  6646.000000   \n",
       "mean          4.799901           189.582975    14.920355     1.132338   \n",
       "std           1.964154            58.126639     9.626527     0.039333   \n",
       "min           1.372222            47.883333    -6.044444     1.036111   \n",
       "25%           3.534722           142.598611     7.540278     1.106111   \n",
       "50%           4.477778           191.908333    14.213889     1.131667   \n",
       "75%           5.544444           232.043056    21.062500     1.160556   \n",
       "max          15.044444           329.544444    41.044444     1.229444   \n",
       "\n",
       "          pressure  precipitation    wind_gust    radiation   wind_speed  \\\n",
       "count  6646.000000    6646.000000  6646.000000  6646.000000  6646.000000   \n",
       "mean    935.286695       0.117364     4.872509   225.250182     3.220986   \n",
       "std       6.105083       0.245561     2.428587   266.299815     1.319704   \n",
       "min     913.800000       0.000000     1.194444     0.000000     0.855556   \n",
       "25%     932.034722       0.000000     3.284722     0.000000     2.294444   \n",
       "50%     934.761111       0.000000     4.327778    95.000000     2.961111   \n",
       "75%     937.805556       0.100000     5.627778   416.422222     3.850000   \n",
       "max     961.238889       1.700000    16.705556   954.288889     9.738889   \n",
       "\n",
       "       wind_direction        price  \n",
       "count     6646.000000  6646.000000  \n",
       "mean       189.082629    50.153506  \n",
       "std         56.712796    12.448392  \n",
       "min         50.538889     4.000000  \n",
       "25%        143.750000    43.000000  \n",
       "50%        191.208333    50.890000  \n",
       "75%        230.865278    59.950000  \n",
       "max        327.711111    85.050000  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CSV_COLUMNS = ['prediction_date', 'wind_speed_100m', \n",
    "  'wind_direction_100m', \n",
    "  'temperature', \n",
    "  'air_density',\n",
    "  'pressure', 'precipitation', 'wind_gust', 'radiation', \n",
    "  'wind_speed', 'wind_direction', 'price']\n",
    "FEATURES = CSV_COLUMNS[1:len(CSV_COLUMNS) - 1]\n",
    "LABEL = CSV_COLUMNS[0]\n",
    "\n",
    "df = pd.read_csv('/content/datalab/energy_forcasing/Sarah/sarah_data.csv', header = None, names = CSV_COLUMNS)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_csv(filename, N):\n",
    "  with open(filename, 'w') as ofp:\n",
    "    for lineno in xrange(0, N):\n",
    "      seq = df.price\n",
    "      line = \",\".join(map(str, seq))\n",
    "      ofp.write(line + '\\n')\n",
    "\n",
    "import os\n",
    "try:\n",
    "  os.makedirs('CNN_W2P_model/')\n",
    "except OSError:\n",
    "  pass\n",
    "to_csv('CNN_W2P_model/train-1.csv', 1000)  # 1000 sequences\n",
    "to_csv('CNN_W2P_model/valid-1.csv', 250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prediction_date</th>\n",
       "      <th>wind_speed_100m</th>\n",
       "      <th>wind_direction_100m</th>\n",
       "      <th>temperature</th>\n",
       "      <th>air_density</th>\n",
       "      <th>pressure</th>\n",
       "      <th>precipitation</th>\n",
       "      <th>wind_gust</th>\n",
       "      <th>radiation</th>\n",
       "      <th>wind_speed</th>\n",
       "      <th>wind_direction</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.000000e+03</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1.000000e+03</td>\n",
       "      <td>1.000000e+03</td>\n",
       "      <td>1000.00</td>\n",
       "      <td>1000.00</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.00</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1.000000e+03</td>\n",
       "      <td>1000.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5.260000e+01</td>\n",
       "      <td>45.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>4.466000e+01</td>\n",
       "      <td>4.085000e+01</td>\n",
       "      <td>50.83</td>\n",
       "      <td>50.83</td>\n",
       "      <td>44.0</td>\n",
       "      <td>50.83</td>\n",
       "      <td>48.0</td>\n",
       "      <td>4.422000e+01</td>\n",
       "      <td>49.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.421797e-14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.421797e-14</td>\n",
       "      <td>1.421797e-14</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.421797e-14</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>5.260000e+01</td>\n",
       "      <td>45.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>4.466000e+01</td>\n",
       "      <td>4.085000e+01</td>\n",
       "      <td>50.83</td>\n",
       "      <td>50.83</td>\n",
       "      <td>44.0</td>\n",
       "      <td>50.83</td>\n",
       "      <td>48.0</td>\n",
       "      <td>4.422000e+01</td>\n",
       "      <td>49.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>5.260000e+01</td>\n",
       "      <td>45.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>4.466000e+01</td>\n",
       "      <td>4.085000e+01</td>\n",
       "      <td>50.83</td>\n",
       "      <td>50.83</td>\n",
       "      <td>44.0</td>\n",
       "      <td>50.83</td>\n",
       "      <td>48.0</td>\n",
       "      <td>4.422000e+01</td>\n",
       "      <td>49.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.260000e+01</td>\n",
       "      <td>45.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>4.466000e+01</td>\n",
       "      <td>4.085000e+01</td>\n",
       "      <td>50.83</td>\n",
       "      <td>50.83</td>\n",
       "      <td>44.0</td>\n",
       "      <td>50.83</td>\n",
       "      <td>48.0</td>\n",
       "      <td>4.422000e+01</td>\n",
       "      <td>49.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5.260000e+01</td>\n",
       "      <td>45.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>4.466000e+01</td>\n",
       "      <td>4.085000e+01</td>\n",
       "      <td>50.83</td>\n",
       "      <td>50.83</td>\n",
       "      <td>44.0</td>\n",
       "      <td>50.83</td>\n",
       "      <td>48.0</td>\n",
       "      <td>4.422000e+01</td>\n",
       "      <td>49.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.260000e+01</td>\n",
       "      <td>45.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>4.466000e+01</td>\n",
       "      <td>4.085000e+01</td>\n",
       "      <td>50.83</td>\n",
       "      <td>50.83</td>\n",
       "      <td>44.0</td>\n",
       "      <td>50.83</td>\n",
       "      <td>48.0</td>\n",
       "      <td>4.422000e+01</td>\n",
       "      <td>49.75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       prediction_date  wind_speed_100m  wind_direction_100m   temperature  \\\n",
       "count     1.000000e+03           1000.0               1000.0  1.000000e+03   \n",
       "mean      5.260000e+01             45.0                 49.0  4.466000e+01   \n",
       "std       1.421797e-14              0.0                  0.0  1.421797e-14   \n",
       "min       5.260000e+01             45.0                 49.0  4.466000e+01   \n",
       "25%       5.260000e+01             45.0                 49.0  4.466000e+01   \n",
       "50%       5.260000e+01             45.0                 49.0  4.466000e+01   \n",
       "75%       5.260000e+01             45.0                 49.0  4.466000e+01   \n",
       "max       5.260000e+01             45.0                 49.0  4.466000e+01   \n",
       "\n",
       "        air_density  pressure  precipitation  wind_gust  radiation  \\\n",
       "count  1.000000e+03   1000.00        1000.00     1000.0    1000.00   \n",
       "mean   4.085000e+01     50.83          50.83       44.0      50.83   \n",
       "std    1.421797e-14      0.00           0.00        0.0       0.00   \n",
       "min    4.085000e+01     50.83          50.83       44.0      50.83   \n",
       "25%    4.085000e+01     50.83          50.83       44.0      50.83   \n",
       "50%    4.085000e+01     50.83          50.83       44.0      50.83   \n",
       "75%    4.085000e+01     50.83          50.83       44.0      50.83   \n",
       "max    4.085000e+01     50.83          50.83       44.0      50.83   \n",
       "\n",
       "       wind_speed  wind_direction    price  \n",
       "count      1000.0    1.000000e+03  1000.00  \n",
       "mean         48.0    4.422000e+01    49.75  \n",
       "std           0.0    1.421797e-14     0.00  \n",
       "min          48.0    4.422000e+01    49.75  \n",
       "25%          48.0    4.422000e+01    49.75  \n",
       "50%          48.0    4.422000e+01    49.75  \n",
       "75%          48.0    4.422000e+01    49.75  \n",
       "max          48.0    4.422000e+01    49.75  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.read_csv('/content/datalab/energy_forcasing/Sarah/CNN_W2P_model/train-1.csv', header = None, names = CSV_COLUMNS)\n",
    "df2.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "rm -rf CNN_W2P_model/trained\n",
    "mkdir CNN_W2P_model/trained\n",
    "touch CNN_W2P_model/trained/__init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /content/datalab/energy_forcasing/Sarah/CNN_W2P_model/trainer/model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /content/datalab/energy_forcasing/Sarah/CNN_W2P_model/trainer/model.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "# Copyright 2017 Google Inc. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "TIMESERIES_COL = 'height'\n",
    "N_OUTPUTS = 1  # in each sequence, 1-49 are features, and 50 is label\n",
    "SEQ_LEN = None\n",
    "DEFAULTS = None\n",
    "N_INPUTS = None\n",
    "\n",
    "\n",
    "def init(hparams):\n",
    "    global SEQ_LEN, DEFAULTS, N_INPUTS\n",
    "    SEQ_LEN = hparams['sequence_length']\n",
    "    DEFAULTS = [[0.0] for x in range(0, SEQ_LEN)]\n",
    "    N_INPUTS = SEQ_LEN - N_OUTPUTS\n",
    "\n",
    "\n",
    "def linear_model(features, mode, params):\n",
    "    X = features[TIMESERIES_COL]\n",
    "    predictions = tf.layers.dense(X, 1, activation=None)\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def dnn_model(features, mode, params):\n",
    "    X = features[TIMESERIES_COL]\n",
    "    h1 = tf.layers.dense(X, 10, activation=tf.nn.relu)\n",
    "    h2 = tf.layers.dense(h1, 3, activation=tf.nn.relu)\n",
    "    predictions = tf.layers.dense(h2, 1, activation=None)  # linear output: regression\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def cnn_model(features, mode, params):\n",
    "    X = tf.reshape(features[TIMESERIES_COL],\n",
    "                   [-1, N_INPUTS, 1])  # as a 1D \"sequence\" with only one time-series observation (height)\n",
    "    c1 = tf.layers.conv1d(X, filters=N_INPUTS // 2,\n",
    "                          kernel_size=3, strides=1,\n",
    "                          padding='same', activation=tf.nn.relu)\n",
    "    p1 = tf.layers.max_pooling1d(c1, pool_size=2, strides=2)\n",
    "\n",
    "    c2 = tf.layers.conv1d(p1, filters=N_INPUTS // 2,\n",
    "                          kernel_size=3, strides=1,\n",
    "                          padding='same', activation=tf.nn.relu)\n",
    "    p2 = tf.layers.max_pooling1d(c2, pool_size=2, strides=2)\n",
    "\n",
    "    outlen = p2.shape[1] * p2.shape[2]\n",
    "    c2flat = tf.reshape(p2, [-1, outlen])\n",
    "    h1 = tf.layers.dense(c2flat, 3, activation=tf.nn.relu)\n",
    "    predictions = tf.layers.dense(h1, 1, activation=None)  # linear output: regression\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def rnn_model(features, mode, params):\n",
    "    CELL_SIZE = N_INPUTS // 3  # size of the internal state in each of the cells\n",
    "\n",
    "    # 1. dynamic_rnn needs 3D shape: [BATCH_SIZE, N_INPUTS, 1]\n",
    "    x = tf.reshape(features[TIMESERIES_COL], [-1, N_INPUTS, 1])\n",
    "\n",
    "    # 2. configure the RNN\n",
    "    cell = tf.nn.rnn_cell.GRUCell(CELL_SIZE)\n",
    "    outputs, state = tf.nn.dynamic_rnn(cell, x, dtype=tf.float32)\n",
    "\n",
    "    # 3. pass rnn output through a dense layer\n",
    "    h1 = tf.layers.dense(state, N_INPUTS // 2, activation=tf.nn.relu)\n",
    "    predictions = tf.layers.dense(h1, 1, activation=None)  # (?, 1)\n",
    "    return predictions\n",
    "\n",
    "\n",
    "# 2-layer RNN\n",
    "def rnn2_model(features, mode, params):\n",
    "    # dynamic_rnn needs 3D shape: [BATCH_SIZE, N_INPUTS, 1]\n",
    "    x = tf.reshape(features[TIMESERIES_COL], [-1, N_INPUTS, 1])\n",
    "\n",
    "    # 2. configure the RNN\n",
    "    cell1 = tf.nn.rnn_cell.GRUCell(N_INPUTS * 2)\n",
    "    cell2 = tf.nn.rnn_cell.GRUCell(N_INPUTS // 2)\n",
    "    cells = tf.nn.rnn_cell.MultiRNNCell([cell1, cell2])\n",
    "    outputs, state = tf.nn.dynamic_rnn(cells, x, dtype=tf.float32)\n",
    "    # 'state' is now a tuple containing the final state of each cell layer\n",
    "    # we use state[1] below to extract the final state of the final layer\n",
    "    \n",
    "    # 3. pass rnn output through a dense layer\n",
    "    h1 = tf.layers.dense(state[1], cells.output_size // 2, activation=tf.nn.relu)\n",
    "    predictions = tf.layers.dense(h1, 1, activation=None)  # (?, 1)\n",
    "    return predictions\n",
    "\n",
    "\n",
    "# create N-1 predictions\n",
    "def rnnN_model(features, mode, params):\n",
    "    # dynamic_rnn needs 3D shape: [BATCH_SIZE, N_INPUTS, 1]\n",
    "    x = tf.reshape(features[TIMESERIES_COL], [-1, N_INPUTS, 1])\n",
    "\n",
    "    # 2. configure the RNN\n",
    "    cell1 = tf.nn.rnn_cell.GRUCell(N_INPUTS * 2)\n",
    "    cell2 = tf.nn.rnn_cell.GRUCell(N_INPUTS // 2)\n",
    "    cells = tf.nn.rnn_cell.MultiRNNCell([cell1, cell2])\n",
    "    outputs, state = tf.nn.dynamic_rnn(cells, x, dtype=tf.float32)\n",
    "    # 'outputs' contains the state of the final layer for every time step\n",
    "    # not just the last time step (?,N_INPUTS, final cell size)\n",
    "    \n",
    "    # 3. pass state for each time step through a DNN, to get a prediction\n",
    "    # for each time step \n",
    "    h1 = tf.layers.dense(outputs, cells.output_size, activation=tf.nn.relu)\n",
    "    h2 = tf.layers.dense(h1, cells.output_size // 2, activation=tf.nn.relu)\n",
    "    predictions = tf.layers.dense(h2, 1, activation=None)  # (?, N_INPUTS, 1)\n",
    "    predictions = tf.reshape(predictions, [-1, N_INPUTS])\n",
    "    return predictions # return prediction for each time step\n",
    "\n",
    "\n",
    "# read data and convert to needed format\n",
    "def read_dataset(filename, mode, batch_size=512):\n",
    "    def _input_fn():\n",
    "        def decode_csv(row):\n",
    "            # row is a string tensor containing the contents of one row\n",
    "            features = tf.decode_csv(row, record_defaults=DEFAULTS)  # string tensor -> list of 50 rank 0 float tensors\n",
    "            label = features.pop()  # remove last feature and use as label\n",
    "            features = tf.stack(features)  # list of rank 0 tensors -> single rank 1 tensor\n",
    "            return {TIMESERIES_COL: features}, label\n",
    "\n",
    "        # Create list of file names that match \"glob\" pattern (i.e. data_file_*.csv)\n",
    "        dataset = tf.data.Dataset.list_files(filename)\n",
    "        # Read in data from files\n",
    "        dataset = dataset.flat_map(tf.data.TextLineDataset)\n",
    "        # Parse text lines as comma-separated values (CSV)\n",
    "        dataset = dataset.map(decode_csv)\n",
    "\n",
    "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "            num_epochs = None  # loop indefinitely\n",
    "            dataset = dataset.shuffle(buffer_size=10 * batch_size)\n",
    "        else:\n",
    "            num_epochs = 1  # end-of-input after this\n",
    "\n",
    "        dataset = dataset.repeat(num_epochs).batch(batch_size)\n",
    "        return dataset.make_one_shot_iterator().get_next()\n",
    "\n",
    "    return _input_fn\n",
    "\n",
    "\n",
    "def serving_input_fn():\n",
    "    feature_placeholders = {\n",
    "        TIMESERIES_COL: tf.placeholder(tf.float32, [None, N_INPUTS])\n",
    "    }\n",
    "\n",
    "    features = {\n",
    "        key: tf.expand_dims(tensor, -1)\n",
    "        for key, tensor in feature_placeholders.items()\n",
    "    }\n",
    "    features[TIMESERIES_COL] = tf.squeeze(features[TIMESERIES_COL], axis=[2])\n",
    "\n",
    "    return tf.estimator.export.ServingInputReceiver(features, feature_placeholders)\n",
    "\n",
    "\n",
    "def compute_errors(features, labels, predictions):\n",
    "    labels = tf.expand_dims(labels, -1)  # rank 1 -> rank 2 to match rank of predictions\n",
    "\n",
    "    if predictions.shape[1] == 1:\n",
    "        loss = tf.losses.mean_squared_error(labels, predictions)\n",
    "        rmse = tf.metrics.root_mean_squared_error(labels, predictions)\n",
    "        return loss, rmse\n",
    "    else:\n",
    "        # one prediction for every input in sequence\n",
    "        # get 1-N of (x + label)\n",
    "        labelsN = tf.concat([features[TIMESERIES_COL], labels], axis=1)\n",
    "        labelsN = labelsN[:, 1:]\n",
    "        # loss is computed from the last 1/3 of the series\n",
    "        N = (2 * N_INPUTS) // 3\n",
    "        loss = tf.losses.mean_squared_error(labelsN[:, N:], predictions[:, N:])\n",
    "        # rmse is computed from last prediction and last label\n",
    "        lastPred = predictions[:, -1]\n",
    "        rmse = tf.metrics.root_mean_squared_error(labels, lastPred)\n",
    "        return loss, rmse\n",
    "\n",
    "# RMSE when predicting same as last value\n",
    "def same_as_last_benchmark(features, labels):\n",
    "    predictions = features[TIMESERIES_COL][:,-1] # last value in input sequence\n",
    "    return tf.metrics.root_mean_squared_error(labels, predictions)\n",
    "\n",
    "\n",
    "# create the inference model\n",
    "def sequence_regressor(features, labels, mode, params):\n",
    "    # 1. run the appropriate model\n",
    "    model_functions = {\n",
    "        'linear': linear_model,\n",
    "        'dnn': dnn_model,\n",
    "        'cnn': cnn_model,\n",
    "        'rnn': rnn_model,\n",
    "        'rnn2': rnn2_model,\n",
    "        'rnnN': rnnN_model}\n",
    "    model_function = model_functions[params['model']]\n",
    "    predictions = model_function(features, mode, params)\n",
    "\n",
    "    # 2. loss function, training/eval ops\n",
    "    loss = None\n",
    "    train_op = None\n",
    "    eval_metric_ops = None\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN or mode == tf.estimator.ModeKeys.EVAL:\n",
    "        loss, rmse = compute_errors(features, labels, predictions)\n",
    "\n",
    "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "            # this is needed for batch normalization, but has no effect otherwise\n",
    "            update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "            with tf.control_dependencies(update_ops):\n",
    "                # 2b. set up training operation\n",
    "                train_op = tf.contrib.layers.optimize_loss(\n",
    "                    loss,\n",
    "                    tf.train.get_global_step(),\n",
    "                    learning_rate=params['learning_rate'],\n",
    "                    optimizer=\"Adam\")\n",
    "\n",
    "        # 2c. eval metric\n",
    "        eval_metric_ops = {\n",
    "            \"RMSE\": rmse,\n",
    "            \"RMSE_same_as_last\": same_as_last_benchmark(features, labels),\n",
    "        }\n",
    "\n",
    "    # 3. Create predictions\n",
    "    if predictions.shape[1] != 1:\n",
    "        predictions = predictions[:, -1]  # last predicted value\n",
    "    predictions_dict = {\"predicted\": predictions}\n",
    "\n",
    "    # 4. return EstimatorSpec\n",
    "    return tf.estimator.EstimatorSpec(\n",
    "        mode=mode,\n",
    "        predictions=predictions_dict,\n",
    "        loss=loss,\n",
    "        train_op=train_op,\n",
    "        eval_metric_ops=eval_metric_ops,\n",
    "        export_outputs={\n",
    "            'predictions': tf.estimator.export.PredictOutput(predictions_dict)}\n",
    "    )\n",
    "\n",
    "\n",
    "def train_and_evaluate(output_dir, hparams):\n",
    "    get_train = read_dataset(hparams['train_data_path'],\n",
    "                             tf.estimator.ModeKeys.TRAIN,\n",
    "                             hparams['train_batch_size'])\n",
    "    get_valid = read_dataset(hparams['eval_data_path'],\n",
    "                             tf.estimator.ModeKeys.EVAL,\n",
    "                             1000)\n",
    "    estimator = tf.estimator.Estimator(model_fn=sequence_regressor,\n",
    "                                       params=hparams,\n",
    "                                       config=tf.estimator.RunConfig(\n",
    "                                           save_checkpoints_secs=\n",
    "                                           hparams['min_eval_frequency']),\n",
    "                                       model_dir=output_dir)\n",
    "    train_spec = tf.estimator.TrainSpec(input_fn=get_train,\n",
    "                                        max_steps=hparams['train_steps'])\n",
    "    exporter = tf.estimator.LatestExporter('exporter', serving_input_fn)\n",
    "    eval_spec = tf.estimator.EvalSpec(input_fn=get_valid,\n",
    "                                      steps=None,\n",
    "                                      exporters=exporter,\n",
    "                                      start_delay_secs=hparams['eval_delay_secs'],\n",
    "                                      throttle_secs=hparams['min_eval_frequency'])\n",
    "    tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /content/datalab/energy_forcasing/Sarah/CNN_W2P_model/trainer/task.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /content/datalab/energy_forcasing/Sarah/CNN_W2P_model/trainer/task.py\n",
    "\n",
    "# Copyright 2017 Google Inc. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "\"\"\"Example implementation of code to run on the Cloud ML service.\n",
    "\"\"\"\n",
    "\n",
    "import traceback\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "from . import model\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  parser = argparse.ArgumentParser()\n",
    "  # Input Arguments\n",
    "  parser.add_argument(\n",
    "      '--train_data_path',\n",
    "      help='GCS or local path to training data',\n",
    "      required=True\n",
    "  )\n",
    "  parser.add_argument(\n",
    "      '--eval_data_path',\n",
    "      help='GCS or local path to evaluation data',\n",
    "      required=True\n",
    "  )\n",
    "  parser.add_argument(\n",
    "      '--train_batch_size',\n",
    "      help='Batch size for training steps',\n",
    "      type=int,\n",
    "      default=100\n",
    "  )\n",
    "  parser.add_argument(\n",
    "      '--learning_rate',\n",
    "      help='Initial learning rate for training',\n",
    "      type=float,\n",
    "      default=0.01\n",
    "  )\n",
    "  parser.add_argument(\n",
    "      '--train_steps',\n",
    "      help=\"\"\"\\\n",
    "      Steps to run the training job for. A step is one batch-size,\\\n",
    "      \"\"\",\n",
    "      type=int,\n",
    "      default=0\n",
    "  )\n",
    "  parser.add_argument(\n",
    "      '--sequence_length',\n",
    "      help=\"\"\"\\\n",
    "      This model works with fixed length sequences. 1-(N-1) are inputs, last is output\n",
    "      \"\"\",\n",
    "      type=int,\n",
    "      default=10\n",
    "  )\n",
    "  parser.add_argument(\n",
    "      '--output_dir',\n",
    "      help='GCS location to write checkpoints and export models',\n",
    "      required=True\n",
    "  )\n",
    "  model_names = [name.replace('_model','') \\\n",
    "                   for name in dir(model) \\\n",
    "                     if name.endswith('_model')]\n",
    "  parser.add_argument(\n",
    "      '--model',\n",
    "      help='Type of model. Supported types are {}'.format(model_names),\n",
    "      required=True\n",
    "  )\n",
    "  parser.add_argument(\n",
    "      '--job-dir',\n",
    "      help='this model ignores this field, but it is required by gcloud',\n",
    "      default='junk'\n",
    "  )\n",
    "  parser.add_argument(\n",
    "      '--eval_delay_secs',\n",
    "      help='How long to wait before running first evaluation',\n",
    "      default=10,\n",
    "      type=int\n",
    "  )\n",
    "  parser.add_argument(\n",
    "      '--min_eval_frequency',\n",
    "      help='Minimum number of training steps between evaluations',\n",
    "      default=60,\n",
    "      type=int\n",
    "  )\n",
    "\n",
    "  args = parser.parse_args()\n",
    "  hparams = args.__dict__\n",
    "  \n",
    "  # unused args provided by service\n",
    "  hparams.pop('job_dir', None)\n",
    "  hparams.pop('job-dir', None)\n",
    "\n",
    "  output_dir = hparams.pop('output_dir')\n",
    "\n",
    "  # Append trial_id to path if we are doing hptuning\n",
    "  # This code can be removed if you are not using hyperparameter tuning\n",
    "  output_dir = os.path.join(\n",
    "      output_dir,\n",
    "      json.loads(\n",
    "          os.environ.get('TF_CONFIG', '{}')\n",
    "      ).get('task', {}).get('trial', '')\n",
    "  )\n",
    "\n",
    "  # calculate train_steps if not provided\n",
    "  if hparams['train_steps'] < 1:\n",
    "     # 1,000 steps at batch_size of 100\n",
    "     hparams['train_steps'] = (1000 * 100) // hparams['train_batch_size']\n",
    "     print (\"Training for {} steps\".format(hparams['train_steps']))\n",
    "\n",
    "  model.init(hparams)\n",
    "\n",
    "  # Run the training job\n",
    "  model.train_and_evaluate(output_dir, hparams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/datalab/energy_forcasing/Sarah\n",
      "BIG_q.ipynb\t\tCNN_W2P_model  Energy_ts\n",
      "Build_model_task.ipynb\tdata\t       sarah_data.csv\n"
     ]
    }
   ],
   "source": [
    "! pwd\n",
    "! ls\n",
    "## /content/datalab/energy_forcasing/Sarah/CNN_W2P_model/trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/envs/py2env/bin/python: No module named CNN_W2P_model\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "DATADIR=$(pwd)/CNN_W2P_model\n",
    "OUTDIR=$(pwd)/CNN_W2P_model/trained\n",
    "rm -rf $OUTDIR\n",
    "gcloud ml-engine local train \\\n",
    "   --module-name=CNN_W2P_model.task \\\n",
    "   --package-path=${PWD}/CNN_W2P_model/trainer \\\n",
    "   -- \\\n",
    "   --train_data_path=\"${DATADIR}/train-1.csv\" \\\n",
    "   --eval_data_path=\"${DATADIR}/valid-1.csv\"  \\\n",
    "   --output_dir=${OUTDIR} \\\n",
    "   --model=linear --train_steps=10 --sequence_length=$SEQ_LEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
