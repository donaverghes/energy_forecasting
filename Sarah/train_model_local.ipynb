{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math \n",
    "\n",
    "PROJECT = 'qwiklabs-gcp-aebfb78fe0f1b1d1' # REPLACE WITH YOUR PROJECT ID\n",
    "BUCKET = 'sarah-bucket' # REPLACE WITH YOUR BUCKET NAME\n",
    "REGION = 'us-central1' # REPLACE WITH YOUR BUCKET REGION e.g. us-central1\n",
    "os.environ['TFVERSION'] = '1.8'  # Tensorflow version\n",
    "\n",
    "SEQ_LEN = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the project, Bucket and Region and set them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for bash\n",
    "os.environ['PROJECT'] = PROJECT\n",
    "os.environ['BUCKET'] = BUCKET\n",
    "os.environ['REGION'] = REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n",
      "Updated property [compute/region].\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gcloud config set project $PROJECT\n",
    "gcloud config set compute/region $REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/datalab/energy_forcasing/Sarah\n",
      "ARIMA.csv\t\t data\t\t  trained\n",
      "BIG_q.ipynb\t\t Energy_ts\t  train_model_local.ipynb\n",
      "build_local_model.ipynb  hyperparam.yaml  Untitled Notebook.ipynb\n",
      "Build_model_task.ipynb\t sarah_data.csv\n",
      "CNN_W2P_model\t\t test.json\n"
     ]
    }
   ],
   "source": [
    "! pwd\n",
    "! ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.948333</td>\n",
       "      <td>0.942500</td>\n",
       "      <td>0.912333</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.781000</td>\n",
       "      <td>0.654167</td>\n",
       "      <td>0.808333</td>\n",
       "      <td>0.724833</td>\n",
       "      <td>0.992833</td>\n",
       "      <td>0.668833</td>\n",
       "      <td>...</td>\n",
       "      <td>0.658333</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.085000</td>\n",
       "      <td>0.953333</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.846000</td>\n",
       "      <td>0.784667</td>\n",
       "      <td>0.751667</td>\n",
       "      <td>0.247333</td>\n",
       "      <td>1.001667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.942500</td>\n",
       "      <td>0.912333</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.781000</td>\n",
       "      <td>0.654167</td>\n",
       "      <td>0.808333</td>\n",
       "      <td>0.724833</td>\n",
       "      <td>0.992833</td>\n",
       "      <td>0.668833</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.085000</td>\n",
       "      <td>0.953333</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.846000</td>\n",
       "      <td>0.784667</td>\n",
       "      <td>0.751667</td>\n",
       "      <td>0.247333</td>\n",
       "      <td>1.001667</td>\n",
       "      <td>1.008333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.912333</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.781000</td>\n",
       "      <td>0.654167</td>\n",
       "      <td>0.808333</td>\n",
       "      <td>0.724833</td>\n",
       "      <td>0.992833</td>\n",
       "      <td>0.668833</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.830833</td>\n",
       "      <td>...</td>\n",
       "      <td>1.085000</td>\n",
       "      <td>0.953333</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.846000</td>\n",
       "      <td>0.784667</td>\n",
       "      <td>0.751667</td>\n",
       "      <td>0.247333</td>\n",
       "      <td>1.001667</td>\n",
       "      <td>1.008333</td>\n",
       "      <td>0.790500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.781000</td>\n",
       "      <td>0.654167</td>\n",
       "      <td>0.808333</td>\n",
       "      <td>0.724833</td>\n",
       "      <td>0.992833</td>\n",
       "      <td>0.668833</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.830833</td>\n",
       "      <td>0.834500</td>\n",
       "      <td>...</td>\n",
       "      <td>0.953333</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.846000</td>\n",
       "      <td>0.784667</td>\n",
       "      <td>0.751667</td>\n",
       "      <td>0.247333</td>\n",
       "      <td>1.001667</td>\n",
       "      <td>1.008333</td>\n",
       "      <td>0.790500</td>\n",
       "      <td>1.124667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.781000</td>\n",
       "      <td>0.654167</td>\n",
       "      <td>0.808333</td>\n",
       "      <td>0.724833</td>\n",
       "      <td>0.992833</td>\n",
       "      <td>0.668833</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.830833</td>\n",
       "      <td>0.834500</td>\n",
       "      <td>0.816667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.846000</td>\n",
       "      <td>0.784667</td>\n",
       "      <td>0.751667</td>\n",
       "      <td>0.247333</td>\n",
       "      <td>1.001667</td>\n",
       "      <td>1.008333</td>\n",
       "      <td>0.790500</td>\n",
       "      <td>1.124667</td>\n",
       "      <td>0.492167</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0  0.948333  0.942500  0.912333  0.500000  0.781000  0.654167  0.808333   \n",
       "1  0.942500  0.912333  0.500000  0.781000  0.654167  0.808333  0.724833   \n",
       "2  0.912333  0.500000  0.781000  0.654167  0.808333  0.724833  0.992833   \n",
       "3  0.500000  0.781000  0.654167  0.808333  0.724833  0.992833  0.668833   \n",
       "4  0.781000  0.654167  0.808333  0.724833  0.992833  0.668833  0.750000   \n",
       "\n",
       "         7         8         9     ...           40        41        42  \\\n",
       "0  0.724833  0.992833  0.668833    ...     0.658333  0.500000  1.085000   \n",
       "1  0.992833  0.668833  0.750000    ...     0.500000  1.085000  0.953333   \n",
       "2  0.668833  0.750000  0.830833    ...     1.085000  0.953333  0.500000   \n",
       "3  0.750000  0.830833  0.834500    ...     0.953333  0.500000  0.846000   \n",
       "4  0.830833  0.834500  0.816667    ...     0.500000  0.846000  0.784667   \n",
       "\n",
       "         43        44        45        46        47        48        49  \n",
       "0  0.953333  0.500000  0.846000  0.784667  0.751667  0.247333  1.001667  \n",
       "1  0.500000  0.846000  0.784667  0.751667  0.247333  1.001667  1.008333  \n",
       "2  0.846000  0.784667  0.751667  0.247333  1.001667  1.008333  0.790500  \n",
       "3  0.784667  0.751667  0.247333  1.001667  1.008333  0.790500  1.124667  \n",
       "4  0.751667  0.247333  1.001667  1.008333  0.790500  1.124667  0.492167  \n",
       "\n",
       "[5 rows x 50 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.read_csv('data/CNN_W2P_model/train-1.csv', header = None)\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.893333</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>0.886500</td>\n",
       "      <td>1.018167</td>\n",
       "      <td>0.668667</td>\n",
       "      <td>0.863333</td>\n",
       "      <td>0.718333</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.711500</td>\n",
       "      <td>0.805833</td>\n",
       "      <td>...</td>\n",
       "      <td>0.725000</td>\n",
       "      <td>0.983167</td>\n",
       "      <td>0.946167</td>\n",
       "      <td>0.919833</td>\n",
       "      <td>1.011500</td>\n",
       "      <td>1.116833</td>\n",
       "      <td>1.019833</td>\n",
       "      <td>0.920833</td>\n",
       "      <td>0.794833</td>\n",
       "      <td>1.083333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.550000</td>\n",
       "      <td>0.886500</td>\n",
       "      <td>1.018167</td>\n",
       "      <td>0.668667</td>\n",
       "      <td>0.863333</td>\n",
       "      <td>0.718333</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.711500</td>\n",
       "      <td>0.805833</td>\n",
       "      <td>0.766000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.983167</td>\n",
       "      <td>0.946167</td>\n",
       "      <td>0.919833</td>\n",
       "      <td>1.011500</td>\n",
       "      <td>1.116833</td>\n",
       "      <td>1.019833</td>\n",
       "      <td>0.920833</td>\n",
       "      <td>0.794833</td>\n",
       "      <td>1.083333</td>\n",
       "      <td>0.744167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.886500</td>\n",
       "      <td>1.018167</td>\n",
       "      <td>0.668667</td>\n",
       "      <td>0.863333</td>\n",
       "      <td>0.718333</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.711500</td>\n",
       "      <td>0.805833</td>\n",
       "      <td>0.766000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.946167</td>\n",
       "      <td>0.919833</td>\n",
       "      <td>1.011500</td>\n",
       "      <td>1.116833</td>\n",
       "      <td>1.019833</td>\n",
       "      <td>0.920833</td>\n",
       "      <td>0.794833</td>\n",
       "      <td>1.083333</td>\n",
       "      <td>0.744167</td>\n",
       "      <td>1.001667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.018167</td>\n",
       "      <td>0.668667</td>\n",
       "      <td>0.863333</td>\n",
       "      <td>0.718333</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.711500</td>\n",
       "      <td>0.805833</td>\n",
       "      <td>0.766000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.983333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.919833</td>\n",
       "      <td>1.011500</td>\n",
       "      <td>1.116833</td>\n",
       "      <td>1.019833</td>\n",
       "      <td>0.920833</td>\n",
       "      <td>0.794833</td>\n",
       "      <td>1.083333</td>\n",
       "      <td>0.744167</td>\n",
       "      <td>1.001667</td>\n",
       "      <td>0.993333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.668667</td>\n",
       "      <td>0.863333</td>\n",
       "      <td>0.718333</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.711500</td>\n",
       "      <td>0.805833</td>\n",
       "      <td>0.766000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.983333</td>\n",
       "      <td>0.970833</td>\n",
       "      <td>...</td>\n",
       "      <td>1.011500</td>\n",
       "      <td>1.116833</td>\n",
       "      <td>1.019833</td>\n",
       "      <td>0.920833</td>\n",
       "      <td>0.794833</td>\n",
       "      <td>1.083333</td>\n",
       "      <td>0.744167</td>\n",
       "      <td>1.001667</td>\n",
       "      <td>0.993333</td>\n",
       "      <td>0.865000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0  0.893333  0.550000  0.886500  1.018167  0.668667  0.863333  0.718333   \n",
       "1  0.550000  0.886500  1.018167  0.668667  0.863333  0.718333  0.960000   \n",
       "2  0.886500  1.018167  0.668667  0.863333  0.718333  0.960000  0.711500   \n",
       "3  1.018167  0.668667  0.863333  0.718333  0.960000  0.711500  0.805833   \n",
       "4  0.668667  0.863333  0.718333  0.960000  0.711500  0.805833  0.766000   \n",
       "\n",
       "         7         8         9     ...           40        41        42  \\\n",
       "0  0.960000  0.711500  0.805833    ...     0.725000  0.983167  0.946167   \n",
       "1  0.711500  0.805833  0.766000    ...     0.983167  0.946167  0.919833   \n",
       "2  0.805833  0.766000  1.000000    ...     0.946167  0.919833  1.011500   \n",
       "3  0.766000  1.000000  0.983333    ...     0.919833  1.011500  1.116833   \n",
       "4  1.000000  0.983333  0.970833    ...     1.011500  1.116833  1.019833   \n",
       "\n",
       "         43        44        45        46        47        48        49  \n",
       "0  0.919833  1.011500  1.116833  1.019833  0.920833  0.794833  1.083333  \n",
       "1  1.011500  1.116833  1.019833  0.920833  0.794833  1.083333  0.744167  \n",
       "2  1.116833  1.019833  0.920833  0.794833  1.083333  0.744167  1.001667  \n",
       "3  1.019833  0.920833  0.794833  1.083333  0.744167  1.001667  0.993333  \n",
       "4  0.920833  0.794833  1.083333  0.744167  1.001667  0.993333  0.865000  \n",
       "\n",
       "[5 rows x 50 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.read_csv('data/CNN_W2P_model/valid-1.csv', header = None) #, names = CSV_COLUMNS)\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "rm -rf /content/datalab/energy_forcasing/Sarah/CNN_W2P_model/trainer\n",
    "mkdir /content/datalab/energy_forcasing/Sarah/CNN_W2P_model/trainer\n",
    "touch /content/datalab/energy_forcasing/Sarah/CNN_W2P_model/trainer/__init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /content/datalab/energy_forcasing/Sarah/CNN_W2P_model/trainer/model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /content/datalab/energy_forcasing/Sarah/CNN_W2P_model/trainer/model.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "# Copyright 2017 Google Inc. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "TIMESERIES_COL = 'price'\n",
    "N_OUTPUTS = 1  # in each sequence, 1-49 are features, and 50 is label\n",
    "SEQ_LEN = None\n",
    "DEFAULTS = None\n",
    "N_INPUTS = None\n",
    "\n",
    "\n",
    "def init(hparams):\n",
    "    global SEQ_LEN, DEFAULTS, N_INPUTS\n",
    "    SEQ_LEN = hparams['sequence_length']\n",
    "    DEFAULTS = [[0.0] for x in range(0, SEQ_LEN)]\n",
    "    N_INPUTS = SEQ_LEN - N_OUTPUTS\n",
    "\n",
    "\n",
    "def linear_model(features, mode, params):\n",
    "    X = features[TIMESERIES_COL]\n",
    "    predictions = tf.layers.dense(X, 1, activation=None)\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def dnn_model(features, mode, params):\n",
    "    X = features[TIMESERIES_COL]\n",
    "    h1 = tf.layers.dense(X, 10, activation=tf.nn.relu)\n",
    "    h2 = tf.layers.dense(h1, 3, activation=tf.nn.relu)\n",
    "    predictions = tf.layers.dense(h2, 1, activation=None)  # linear output: regression\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def cnn_model(features, mode, params):\n",
    "    X = tf.reshape(features[TIMESERIES_COL],\n",
    "                   [-1, N_INPUTS, 1])  # as a 1D \"sequence\" with only one time-series observation (height)\n",
    "    c1 = tf.layers.conv1d(X, filters=N_INPUTS // 2,\n",
    "                          kernel_size=3, strides=1,\n",
    "                          padding='same', activation=tf.nn.relu)\n",
    "    p1 = tf.layers.max_pooling1d(c1, pool_size=2, strides=2)\n",
    "\n",
    "#     outlen = p1.shape[1] * p1.shape[2]\n",
    "#     c1flat = tf.reshape(p1, [-1, outlen])\n",
    "#     h1 = tf.layers.dense(c1flat, 3, activation=tf.nn.relu)\n",
    "#     predictions = tf.layers.dense(h1, 1, activation=None)  # linear output: regression\n",
    "#     return predictions\n",
    "    \n",
    "    c2 = tf.layers.conv1d(p1, filters=N_INPUTS // 2,\n",
    "                          kernel_size=3, strides=1,\n",
    "                          padding='same', activation=tf.nn.relu)\n",
    "    p2 = tf.layers.max_pooling1d(c2, pool_size=2, strides=2)\n",
    "\n",
    "    outlen = p2.shape[1] * p2.shape[2]\n",
    "    c2flat = tf.reshape(p2, [-1, outlen])\n",
    "    h1 = tf.layers.dense(c2flat, 3, activation=tf.nn.relu)\n",
    "    predictions = tf.layers.dense(h1, 1, activation=None)  # linear output: regression\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def rnn_model(features, mode, params):\n",
    "    CELL_SIZE = N_INPUTS // 3  # size of the internal state in each of the cells\n",
    "\n",
    "    # 1. dynamic_rnn needs 3D shape: [BATCH_SIZE, N_INPUTS, 1]\n",
    "    x = tf.reshape(features[TIMESERIES_COL], [-1, N_INPUTS, 1])\n",
    "\n",
    "    # 2. configure the RNN\n",
    "    cell = tf.nn.rnn_cell.GRUCell(CELL_SIZE)\n",
    "    outputs, state = tf.nn.dynamic_rnn(cell, x, dtype=tf.float32)\n",
    "\n",
    "    # 3. pass rnn output through a dense layer\n",
    "    h1 = tf.layers.dense(state, N_INPUTS // 2, activation=tf.nn.relu)\n",
    "    h2 = tf.layers.dense(h1, N_INPUTS // 2, activation=tf.nn.relu6)\n",
    "    h3 = tf.layers.dense(h2, N_INPUTS // 2, activation=tf.nn.relu)\n",
    "    \n",
    "    predictions = tf.layers.dense(h3, 1, activation=None)  # (?, 1)\n",
    "    return predictions\n",
    "\n",
    "\n",
    "# 2-layer RNN\n",
    "def rnn2_model(features, mode, params):\n",
    "    # 1. dynamic_rnn needs 3D shape: [BATCH_SIZE, N_INPUTS, 1]\n",
    "    x = tf.reshape(features[TIMESERIES_COL], [-1, N_INPUTS, 1])\n",
    "\n",
    "    # 2. configure the RNN\n",
    "    cell1 = tf.nn.rnn_cell.GRUCell(N_INPUTS * 3)\n",
    "    cell2 = tf.nn.rnn_cell.GRUCell(N_INPUTS // 3)\n",
    "    cells = tf.nn.rnn_cell.MultiRNNCell([cell1, cell2])\n",
    "    outputs, state = tf.nn.dynamic_rnn(cells, x, dtype=tf.float32)\n",
    "    # 'state' is now a tuple containing the final state of each cell layer\n",
    "    # we use state[1] below to extract the final state of the final layer\n",
    "    outputs = outputs[:, (N_INPUTS-1):, :] # last one only\n",
    "  \n",
    "    # 3. pass rnn output through a dense layer\n",
    "    h1 = tf.layers.dense(state[1], cells.output_size // 2, activation=tf.nn.relu)\n",
    "    predictions = tf.layers.dense(h1, 1, activation=None)  # (?, 1)\n",
    "    return predictions\n",
    "\n",
    "\n",
    "# create N-1 predictions\n",
    "def rnnN_model(features, mode, params):\n",
    "    # dynamic_rnn needs 3D shape: [BATCH_SIZE, N_INPUTS, 1]\n",
    "    x = tf.reshape(features[TIMESERIES_COL], [-1, N_INPUTS, 1])\n",
    "\n",
    "    # 2. configure the RNN\n",
    "    cell1 = tf.nn.rnn_cell.GRUCell(N_INPUTS * 2)\n",
    "    cell2 = tf.nn.rnn_cell.GRUCell(N_INPUTS // 2)\n",
    "    cells = tf.nn.rnn_cell.MultiRNNCell([cell1, cell2])\n",
    "    outputs, state = tf.nn.dynamic_rnn(cells, x, dtype=tf.float32)\n",
    "    # 'outputs' contains the state of the final layer for every time step\n",
    "    # not just the last time step (?,N_INPUTS, final cell size)\n",
    "    \n",
    "    # 3. pass state for each time step through a DNN, to get a prediction\n",
    "    # for each time step \n",
    "    h1 = tf.layers.dense(outputs, cells.output_size, activation=tf.nn.relu)\n",
    "    h2 = tf.layers.dense(h1, cells.output_size // 2, activation=tf.nn.relu)\n",
    "    predictions = tf.layers.dense(h2, 1, activation=None)  # (?, N_INPUTS, 1)\n",
    "    predictions = tf.reshape(predictions, [-1, N_INPUTS])\n",
    "    return predictions # return prediction for each time step\n",
    "\n",
    "\n",
    "# read data and convert to needed format\n",
    "def read_dataset(filename, mode, batch_size=512):\n",
    "    def _input_fn():\n",
    "        def decode_csv(row):\n",
    "            # row is a string tensor containing the contents of one row\n",
    "            features = tf.decode_csv(row, record_defaults=DEFAULTS)  # string tensor -> list of 50 rank 0 float tensors\n",
    "            label = features.pop()  # remove last feature and use as label\n",
    "            features = tf.stack(features)  # list of rank 0 tensors -> single rank 1 tensor\n",
    "            return {TIMESERIES_COL: features}, label\n",
    "\n",
    "        # Create list of file names that match \"glob\" pattern (i.e. data_file_*.csv)\n",
    "        dataset = tf.data.Dataset.list_files(filename)\n",
    "        # Read in data from files\n",
    "        dataset = dataset.flat_map(tf.data.TextLineDataset)\n",
    "        # Parse text lines as comma-separated values (CSV)\n",
    "        dataset = dataset.map(decode_csv)\n",
    "\n",
    "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "            num_epochs = None  # loop indefinitely\n",
    "            dataset = dataset.shuffle(buffer_size=10 * batch_size)\n",
    "        else:\n",
    "            num_epochs = 1  # end-of-input after this\n",
    "\n",
    "        dataset = dataset.repeat(num_epochs).batch(batch_size)\n",
    "        return dataset.make_one_shot_iterator().get_next()\n",
    "\n",
    "    return _input_fn\n",
    "\n",
    "\n",
    "def serving_input_fn():\n",
    "    feature_placeholders = {\n",
    "        TIMESERIES_COL: tf.placeholder(tf.float32, [None, N_INPUTS])\n",
    "    }\n",
    "\n",
    "    features = {\n",
    "        key: tf.expand_dims(tensor, -1)\n",
    "        for key, tensor in feature_placeholders.items()\n",
    "    }\n",
    "    features[TIMESERIES_COL] = tf.squeeze(features[TIMESERIES_COL], axis=[2])\n",
    "\n",
    "    return tf.estimator.export.ServingInputReceiver(features, feature_placeholders)\n",
    "\n",
    "\n",
    "def compute_errors(features, labels, predictions):\n",
    "    labels = tf.expand_dims(labels, -1)  # rank 1 -> rank 2 to match rank of predictions\n",
    "\n",
    "    if predictions.shape[1] == 1:\n",
    "        loss = tf.losses.mean_squared_error(labels, predictions)\n",
    "        rmse = tf.metrics.root_mean_squared_error(labels, predictions)\n",
    "        return loss, rmse\n",
    "    else:\n",
    "        # one prediction for every input in sequence\n",
    "        # get 1-N of (x + label)\n",
    "        labelsN = tf.concat([features[TIMESERIES_COL], labels], axis=1)\n",
    "        labelsN = labelsN[:, 1:]\n",
    "        # loss is computed from the last 1/3 of the series\n",
    "        N = (2 * N_INPUTS) // 3\n",
    "        loss = tf.losses.mean_squared_error(labelsN[:, N:], predictions[:, N:])\n",
    "        # rmse is computed from last prediction and last label\n",
    "        lastPred = predictions[:, -1]\n",
    "        rmse = tf.metrics.root_mean_squared_error(labels, lastPred)\n",
    "        return loss, rmse\n",
    "\n",
    "# RMSE when predicting same as last value\n",
    "def same_as_last_benchmark(features, labels):\n",
    "    predictions = features[TIMESERIES_COL][:,-1] # last value in input sequence\n",
    "    return tf.metrics.root_mean_squared_error(labels, predictions)\n",
    "\n",
    "\n",
    "# create the inference model\n",
    "def sequence_regressor(features, labels, mode, params):\n",
    "    # 1. run the appropriate model\n",
    "    model_functions = {\n",
    "        'linear': linear_model,\n",
    "        'dnn': dnn_model,\n",
    "        'cnn': cnn_model,\n",
    "        'rnn': rnn_model,\n",
    "        'rnn2': rnn2_model,\n",
    "        'rnnN': rnnN_model}\n",
    "    model_function = model_functions[params['model']]\n",
    "    print(model_function)\n",
    "    predictions = model_function(features, mode, params)\n",
    "\n",
    "    # 2. loss function, training/eval ops\n",
    "    loss = None\n",
    "    train_op = None\n",
    "    eval_metric_ops = None\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN or mode == tf.estimator.ModeKeys.EVAL:\n",
    "        loss, rmse = compute_errors(features, labels, predictions)\n",
    "    \n",
    "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "            # this is needed for batch normalization, but has no effect otherwise\n",
    "            update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "            with tf.control_dependencies(update_ops):\n",
    "                # 2b. set up training operation\n",
    "                train_op = tf.contrib.layers.optimize_loss(\n",
    "                    loss,\n",
    "                    tf.train.get_global_step(),\n",
    "                    learning_rate=params['learning_rate'],\n",
    "                    optimizer=\"Adam\")\n",
    "\n",
    "        # 2c. eval metric\n",
    "        eval_metric_ops = {\n",
    "            \"RMSE\": rmse,\n",
    "            \"RMSE_same_as_last\": same_as_last_benchmark(features, labels),\n",
    "        }\n",
    "\n",
    "    # 3. Create predictions\n",
    "    if predictions.shape[1] != 1:\n",
    "        predictions = predictions[:, -1]  # last predicted value\n",
    "    predictions_dict = {\"predicted\": predictions}\n",
    "\n",
    "    # 4. return EstimatorSpec\n",
    "    return tf.estimator.EstimatorSpec(\n",
    "        mode=mode,\n",
    "        predictions=predictions_dict,\n",
    "        loss=loss,\n",
    "        train_op=train_op,\n",
    "        eval_metric_ops=eval_metric_ops,\n",
    "        export_outputs={\n",
    "            'predictions': tf.estimator.export.PredictOutput(predictions_dict)}\n",
    "    )\n",
    "\n",
    "\n",
    "def train_and_evaluate(output_dir, hparams):\n",
    "    get_train = read_dataset(hparams['train_data_path'],\n",
    "                             tf.estimator.ModeKeys.TRAIN,\n",
    "                             hparams['train_batch_size'])\n",
    "    get_valid = read_dataset(hparams['eval_data_path'],\n",
    "                             tf.estimator.ModeKeys.EVAL,\n",
    "                             1000)\n",
    "    estimator = tf.estimator.Estimator(model_fn=sequence_regressor,\n",
    "                                       params=hparams,\n",
    "                                       config=tf.estimator.RunConfig(\n",
    "                                           save_checkpoints_steps=50,\n",
    "                                         save_summary_steps=50\n",
    "                                       ),\n",
    "                                       model_dir=output_dir)\n",
    "    train_spec = tf.estimator.TrainSpec(input_fn=get_train,\n",
    "                                        max_steps=hparams['train_steps'])\n",
    "    exporter = tf.estimator.LatestExporter('exporter', serving_input_fn)\n",
    "    eval_spec = tf.estimator.EvalSpec(input_fn=get_valid,\n",
    "                                      steps=None,\n",
    "                                      exporters=exporter,\n",
    "                                      #start_delay_secs=hparams['eval_delay_secs'],\n",
    "                                      #throttle_secs=hparams['min_eval_frequency']\n",
    "                                     )\n",
    "    tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /content/datalab/energy_forcasing/Sarah/CNN_W2P_model/trainer/task.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /content/datalab/energy_forcasing/Sarah/CNN_W2P_model/trainer/task.py\n",
    "\n",
    "# Copyright 2017 Google Inc. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "\"\"\"Example implementation of code to run on the Cloud ML service.\n",
    "\"\"\"\n",
    "\n",
    "import traceback\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "from . import model\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  parser = argparse.ArgumentParser()\n",
    "  # Input Arguments\n",
    "  parser.add_argument(\n",
    "      '--train_data_path',\n",
    "      help='GCS or local path to training data',\n",
    "      required=True\n",
    "  )\n",
    "  parser.add_argument(\n",
    "      '--eval_data_path',\n",
    "      help='GCS or local path to evaluation data',\n",
    "      required=True\n",
    "  )\n",
    "  parser.add_argument(\n",
    "      '--train_batch_size',\n",
    "      help='Batch size for training steps',\n",
    "      type=int,\n",
    "      default=100\n",
    "  )\n",
    "  parser.add_argument(\n",
    "      '--learning_rate',\n",
    "      help='Initial learning rate for training',\n",
    "      type=float,\n",
    "      default=0.01\n",
    "  )\n",
    "  parser.add_argument(\n",
    "      '--train_steps',\n",
    "      help=\"\"\"\\\n",
    "      Steps to run the training job for. A step is one batch-size,\\\n",
    "      \"\"\",\n",
    "      type=int,\n",
    "      default=0\n",
    "  )\n",
    "  parser.add_argument(\n",
    "      '--sequence_length',\n",
    "      help=\"\"\"\\\n",
    "      This model works with fixed length sequences. 1-(N-1) are inputs, last is output\n",
    "      \"\"\",\n",
    "      type=int,\n",
    "      default=10\n",
    "  )\n",
    "  parser.add_argument(\n",
    "      '--output_dir',\n",
    "      help='GCS location to write checkpoints and export models',\n",
    "      required=True\n",
    "  )\n",
    "  model_names = [name.replace('_model','') \\\n",
    "                   for name in dir(model) \\\n",
    "                     if name.endswith('_model')]\n",
    "  parser.add_argument(\n",
    "      '--model',\n",
    "      help='Type of model. Supported types are {}'.format(model_names),\n",
    "      required=True\n",
    "  )\n",
    "  parser.add_argument(\n",
    "      '--job-dir',\n",
    "      help='this model ignores this field, but it is required by gcloud',\n",
    "      default='junk'\n",
    "  )\n",
    "  parser.add_argument(\n",
    "      '--eval_delay_secs',\n",
    "      help='How long to wait before running first evaluation',\n",
    "      default=10,\n",
    "      type=int\n",
    "  )\n",
    "  parser.add_argument(\n",
    "      '--min_eval_frequency',\n",
    "      help='Minimum number of training steps between evaluations',\n",
    "      default=60,\n",
    "      type=int\n",
    "  )\n",
    "\n",
    "  args = parser.parse_args()\n",
    "  hparams = args.__dict__\n",
    "  \n",
    "  # unused args provided by service\n",
    "  hparams.pop('job_dir', None)\n",
    "  hparams.pop('job-dir', None)\n",
    "\n",
    "  output_dir = hparams.pop('output_dir')\n",
    "\n",
    "  # Append trial_id to path if we are doing hptuning\n",
    "  # This code can be removed if you are not using hyperparameter tuning\n",
    "  output_dir = os.path.join(\n",
    "      output_dir,\n",
    "      json.loads(\n",
    "          os.environ.get('TF_CONFIG', '{}')\n",
    "      ).get('task', {}).get('trial', '')\n",
    "  )\n",
    "\n",
    "  # calculate train_steps if not provided\n",
    "  if hparams['train_steps'] < 1:\n",
    "     # 1,000 steps at batch_size of 100\n",
    "     hparams['train_steps'] = (1000 * 100) // hparams['train_batch_size']\n",
    "     print (\"Training for {} steps\".format(hparams['train_steps']))\n",
    "\n",
    "  model.init(hparams)\n",
    "\n",
    "  # Run the training job\n",
    "  model.train_and_evaluate(output_dir, hparams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/datalab/energy_forcasing/Sarah\n",
      "__init__.py  model.py  task.py\n"
     ]
    }
   ],
   "source": [
    "! pwd\n",
    "#! ls\n",
    "#!{PWD}/CNN_W2P_model/trainer\n",
    "\n",
    "! ls /content/datalab/energy_forcasing/Sarah/CNN_W2P_model/trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run the model locally on the vm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function linear_model at 0x7f4fe01b7230>\n",
      "<function linear_model at 0x7f4fe01b7230>\n",
      "<function linear_model at 0x7f4fe01b7230>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/envs/py2env/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "INFO:tensorflow:TF_CONFIG environment variable: {u'environment': u'cloud', u'cluster': {}, u'job': {u'args': [u'--train_data_path=/content/datalab/energy_forcasing/Sarah/data/CNN_W2P_model/train-1.csv', u'--eval_data_path=/content/datalab/energy_forcasing/Sarah/data/CNN_W2P_model/valid-1.csv', u'--output_dir=/content/datalab/energy_forcasing/Sarah/CNN_W2P_model/trained', u'--model=linear', u'--train_steps=2000', u'--sequence_length=50'], u'job_name': u'trainer.task'}, u'task': {}}\n",
      "INFO:tensorflow:Using config: {'_save_checkpoints_secs': None, '_session_config': None, '_keep_checkpoint_max': 5, '_task_type': 'worker', '_train_distribute': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f4fe01aad10>, '_evaluation_master': '', '_save_checkpoints_steps': 50, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_master': '', '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_model_dir': '/content/datalab/energy_forcasing/Sarah/CNN_W2P_model/trained/', '_global_id_in_cluster': 0, '_save_summary_steps': 50}\n",
      "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n",
      "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after 600 secs (eval_spec.throttle_secs) or training is finished.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "2018-09-24 14:23:06.638419: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /content/datalab/energy_forcasing/Sarah/CNN_W2P_model/trained/model.ckpt.\n",
      "INFO:tensorflow:loss = 2.1785707, step = 1\n",
      "INFO:tensorflow:Saving checkpoints for 51 into /content/datalab/energy_forcasing/Sarah/CNN_W2P_model/trained/model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 101 into /content/datalab/energy_forcasing/Sarah/CNN_W2P_model/trained/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 47.5865\n",
      "INFO:tensorflow:loss = 0.120425254, step = 101 (2.102 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 151 into /content/datalab/energy_forcasing/Sarah/CNN_W2P_model/trained/model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 201 into /content/datalab/energy_forcasing/Sarah/CNN_W2P_model/trained/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 50.6617\n",
      "INFO:tensorflow:loss = 0.07700816, step = 201 (1.974 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 251 into /content/datalab/energy_forcasing/Sarah/CNN_W2P_model/trained/model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 301 into /content/datalab/energy_forcasing/Sarah/CNN_W2P_model/trained/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 49.5576\n",
      "INFO:tensorflow:loss = 0.05875025, step = 301 (2.018 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 351 into /content/datalab/energy_forcasing/Sarah/CNN_W2P_model/trained/model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 401 into /content/datalab/energy_forcasing/Sarah/CNN_W2P_model/trained/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 49.1487\n",
      "INFO:tensorflow:loss = 0.07066722, step = 401 (2.035 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 451 into /content/datalab/energy_forcasing/Sarah/CNN_W2P_model/trained/model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 501 into /content/datalab/energy_forcasing/Sarah/CNN_W2P_model/trained/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 50.0909\n",
      "INFO:tensorflow:loss = 0.05105969, step = 501 (1.996 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 551 into /content/datalab/energy_forcasing/Sarah/CNN_W2P_model/trained/model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 601 into /content/datalab/energy_forcasing/Sarah/CNN_W2P_model/trained/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 49.0412\n",
      "INFO:tensorflow:loss = 0.04947878, step = 601 (2.039 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 651 into /content/datalab/energy_forcasing/Sarah/CNN_W2P_model/trained/model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 701 into /content/datalab/energy_forcasing/Sarah/CNN_W2P_model/trained/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 47.4679\n",
      "INFO:tensorflow:loss = 0.044377953, step = 701 (2.107 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 751 into /content/datalab/energy_forcasing/Sarah/CNN_W2P_model/trained/model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 801 into /content/datalab/energy_forcasing/Sarah/CNN_W2P_model/trained/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 50.5787\n",
      "INFO:tensorflow:loss = 0.03577598, step = 801 (1.977 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 851 into /content/datalab/energy_forcasing/Sarah/CNN_W2P_model/trained/model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 901 into /content/datalab/energy_forcasing/Sarah/CNN_W2P_model/trained/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 49.6631\n",
      "INFO:tensorflow:loss = 0.039791692, step = 901 (2.013 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 951 into /content/datalab/energy_forcasing/Sarah/CNN_W2P_model/trained/model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 1001 into /content/datalab/energy_forcasing/Sarah/CNN_W2P_model/trained/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 48.8389\n",
      "INFO:tensorflow:loss = 0.05738052, step = 1001 (2.048 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1051 into /content/datalab/energy_forcasing/Sarah/CNN_W2P_model/trained/model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 1101 into /content/datalab/energy_forcasing/Sarah/CNN_W2P_model/trained/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 49.0024\n",
      "INFO:tensorflow:loss = 0.03196222, step = 1101 (2.041 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1151 into /content/datalab/energy_forcasing/Sarah/CNN_W2P_model/trained/model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 1201 into /content/datalab/energy_forcasing/Sarah/CNN_W2P_model/trained/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 47.6444\n",
      "INFO:tensorflow:loss = 0.02851004, step = 1201 (2.099 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1251 into /content/datalab/energy_forcasing/Sarah/CNN_W2P_model/trained/model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 1301 into /content/datalab/energy_forcasing/Sarah/CNN_W2P_model/trained/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 47.9711\n",
      "INFO:tensorflow:loss = 0.054335948, step = 1301 (2.084 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1351 into /content/datalab/energy_forcasing/Sarah/CNN_W2P_model/trained/model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 1401 into /content/datalab/energy_forcasing/Sarah/CNN_W2P_model/trained/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 48.7167\n",
      "INFO:tensorflow:loss = 0.033797514, step = 1401 (2.053 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1451 into /content/datalab/energy_forcasing/Sarah/CNN_W2P_model/trained/model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 1501 into /content/datalab/energy_forcasing/Sarah/CNN_W2P_model/trained/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 49.5996\n",
      "INFO:tensorflow:loss = 0.048717823, step = 1501 (2.016 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1551 into /content/datalab/energy_forcasing/Sarah/CNN_W2P_model/trained/model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 1601 into /content/datalab/energy_forcasing/Sarah/CNN_W2P_model/trained/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 48.3617\n",
      "INFO:tensorflow:loss = 0.051347278, step = 1601 (2.068 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1651 into /content/datalab/energy_forcasing/Sarah/CNN_W2P_model/trained/model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 1701 into /content/datalab/energy_forcasing/Sarah/CNN_W2P_model/trained/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 49.5685\n",
      "INFO:tensorflow:loss = 0.041464433, step = 1701 (2.017 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1751 into /content/datalab/energy_forcasing/Sarah/CNN_W2P_model/trained/model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 1801 into /content/datalab/energy_forcasing/Sarah/CNN_W2P_model/trained/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 50.1857\n",
      "INFO:tensorflow:loss = 0.04670189, step = 1801 (1.993 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1851 into /content/datalab/energy_forcasing/Sarah/CNN_W2P_model/trained/model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 1901 into /content/datalab/energy_forcasing/Sarah/CNN_W2P_model/trained/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 51.1235\n",
      "INFO:tensorflow:loss = 0.052056774, step = 1901 (1.956 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1951 into /content/datalab/energy_forcasing/Sarah/CNN_W2P_model/trained/model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 2000 into /content/datalab/energy_forcasing/Sarah/CNN_W2P_model/trained/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.03602785.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-09-24-14:23:47\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /content/datalab/energy_forcasing/Sarah/CNN_W2P_model/trained/model.ckpt-2000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-09-24-14:23:48\n",
      "INFO:tensorflow:Saving dict for global step 2000: RMSE = 0.1962908, RMSE_same_as_last = 0.21940677, global_step = 2000, loss = 0.03853008\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['serving_default', 'predictions']\n",
      "INFO:tensorflow:Restoring parameters from /content/datalab/energy_forcasing/Sarah/CNN_W2P_model/trained/model.ckpt-2000\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: /content/datalab/energy_forcasing/Sarah/CNN_W2P_model/trained/export/exporter/temp-1537799028/saved_model.pb\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "DATADIR=$(pwd)/data/CNN_W2P_model \n",
    "OUTDIR=$(pwd)/CNN_W2P_model/trained\n",
    "SEQ_LEN=50\n",
    "rm -rf $OUTDIR\n",
    "gcloud ml-engine local train \\\n",
    "   --module-name=trainer.task \\\n",
    "   --package-path=${PWD}/CNN_W2P_model/trainer \\\n",
    "   -- \\\n",
    "   --train_data_path=\"${DATADIR}/train-1.csv\" \\\n",
    "   --eval_data_path=\"${DATADIR}/valid-1.csv\"  \\\n",
    "   --output_dir=${OUTDIR} \\\n",
    "   --model=linear --train_steps=2000  --sequence_length=$SEQ_LEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the newly created models location after it has been trained and it should be nameed a string of numbers, this time mine is named '1537798159/' whould yours should be a different string of numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34m1537799028\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "ls CNN_W2P_model/trained/export/exporter/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feeding in a string of 49 prices to have the model preduct the 50th price. The data is not scaled down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile test.json\n",
    "[45.52, 45.24, 43.792, 24.0, 37.488, 31.400000000000002, 38.800000000000004, 34.792, 47.656000000000006, 32.104000000000006, 36.0, 39.88, 40.056000000000004, 39.2, 34.952, 50.42400000000001, 36.472, 42.848000000000006, 30.496, 44.712, 32.128, 52.08, 31.664, 47.88, 44.0, 51.352000000000004, 42.624, 54.879999999999995, 32.800000000000004, 31.400000000000002, 30.680000000000003, 44.608000000000004, 32.256, 50.072, 37.424, 39.88, 47.752, 44.800000000000004, 52.08, 46.952, 31.6, 24.0, 52.08, 45.760000000000005, 24.0, 40.608000000000004, 37.664, 36.080000000000005, 11.872]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICTED\n",
      "[27.66790771484375]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: /usr/local/envs/py2env/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "2018-09-24 14:24:48.221923: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "model=$(ls CNN_W2P_model/trained/export/exporter | tail -1)\n",
    "gcloud ml-engine local predict \\\n",
    "    --model-dir=CNN_W2P_model/trained/export/exporter/$model \\\n",
    "    --json-instances=./test.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'predicted' value above should be in stark contrast to the value you will get out when you feed in a string of 0's (as demonstrated below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile test.json\n",
    "{\"price\": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICTED\n",
      "[0.3067505359649658]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: /usr/local/envs/py2env/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "2018-09-24 14:24:55.554682: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "model=$(ls CNN_W2P_model/trained/export/exporter | tail -1)\n",
    "gcloud ml-engine local predict \\\n",
    "    --model-dir=CNN_W2P_model/trained/export/exporter/$model \\\n",
    "    --json-instances=./test.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
