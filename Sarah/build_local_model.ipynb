{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/envs/py2env/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math \n",
    "\n",
    "PROJECT = 'qwiklabs-gcp-aebfb78fe0f1b1d1' # REPLACE WITH YOUR PROJECT ID\n",
    "BUCKET = 'sarah-bucket' # REPLACE WITH YOUR BUCKET NAME\n",
    "REGION = 'us-central1' # REPLACE WITH YOUR BUCKET REGION e.g. us-central1\n",
    "os.environ['TFVERSION'] = '1.8'  # Tensorflow version\n",
    "\n",
    "SEQ_LEN = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the project, Bucket and Region and set them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for bash\n",
    "os.environ['PROJECT'] = PROJECT\n",
    "os.environ['BUCKET'] = BUCKET\n",
    "os.environ['REGION'] = REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n",
      "Updated property [compute/region].\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gcloud config set project $PROJECT\n",
    "gcloud config set compute/region $REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/datalab/energy_forcasing/Sarah\n",
      "ARIMA.csv\t\t Build_model_task.ipynb  Energy_ts\t  test.json\n",
      "BIG_q.ipynb\t\t CNN_W2P_model\t\t hyperparam.yaml  trained\n",
      "build_local_model.ipynb  data\t\t\t sarah_data.csv\n"
     ]
    }
   ],
   "source": [
    "! pwd\n",
    "! ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## move the csv file to the bucket ~ you rly dont need to do this\n",
    "## !gsutil cp -p /content/datalab/energy_forcasing/Sarah/sarah_data.csv gs://sarah-bucket/sarah_data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wind_speed_100m</th>\n",
       "      <th>wind_direction_100m</th>\n",
       "      <th>temperature</th>\n",
       "      <th>air_density</th>\n",
       "      <th>pressure</th>\n",
       "      <th>precipitation</th>\n",
       "      <th>wind_gust</th>\n",
       "      <th>radiation</th>\n",
       "      <th>wind_speed</th>\n",
       "      <th>wind_direction</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>6646.000000</td>\n",
       "      <td>6646.000000</td>\n",
       "      <td>6646.000000</td>\n",
       "      <td>6646.000000</td>\n",
       "      <td>6646.000000</td>\n",
       "      <td>6646.000000</td>\n",
       "      <td>6646.000000</td>\n",
       "      <td>6646.000000</td>\n",
       "      <td>6646.000000</td>\n",
       "      <td>6646.000000</td>\n",
       "      <td>6646.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.799901</td>\n",
       "      <td>189.582975</td>\n",
       "      <td>14.920355</td>\n",
       "      <td>1.132338</td>\n",
       "      <td>935.286695</td>\n",
       "      <td>0.117364</td>\n",
       "      <td>4.872509</td>\n",
       "      <td>225.250182</td>\n",
       "      <td>3.220986</td>\n",
       "      <td>189.082629</td>\n",
       "      <td>50.153506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.964154</td>\n",
       "      <td>58.126639</td>\n",
       "      <td>9.626527</td>\n",
       "      <td>0.039333</td>\n",
       "      <td>6.105083</td>\n",
       "      <td>0.245561</td>\n",
       "      <td>2.428587</td>\n",
       "      <td>266.299815</td>\n",
       "      <td>1.319704</td>\n",
       "      <td>56.712796</td>\n",
       "      <td>12.448392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.372222</td>\n",
       "      <td>47.883333</td>\n",
       "      <td>-6.044444</td>\n",
       "      <td>1.036111</td>\n",
       "      <td>913.800000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.194444</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.855556</td>\n",
       "      <td>50.538889</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.534722</td>\n",
       "      <td>142.598611</td>\n",
       "      <td>7.540278</td>\n",
       "      <td>1.106111</td>\n",
       "      <td>932.034722</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.284722</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.294444</td>\n",
       "      <td>143.750000</td>\n",
       "      <td>43.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4.477778</td>\n",
       "      <td>191.908333</td>\n",
       "      <td>14.213889</td>\n",
       "      <td>1.131667</td>\n",
       "      <td>934.761111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.327778</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>2.961111</td>\n",
       "      <td>191.208333</td>\n",
       "      <td>50.890000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5.544444</td>\n",
       "      <td>232.043056</td>\n",
       "      <td>21.062500</td>\n",
       "      <td>1.160556</td>\n",
       "      <td>937.805556</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>5.627778</td>\n",
       "      <td>416.422222</td>\n",
       "      <td>3.850000</td>\n",
       "      <td>230.865278</td>\n",
       "      <td>59.950000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>15.044444</td>\n",
       "      <td>329.544444</td>\n",
       "      <td>41.044444</td>\n",
       "      <td>1.229444</td>\n",
       "      <td>961.238889</td>\n",
       "      <td>1.700000</td>\n",
       "      <td>16.705556</td>\n",
       "      <td>954.288889</td>\n",
       "      <td>9.738889</td>\n",
       "      <td>327.711111</td>\n",
       "      <td>85.050000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       wind_speed_100m  wind_direction_100m  temperature  air_density  \\\n",
       "count      6646.000000          6646.000000  6646.000000  6646.000000   \n",
       "mean          4.799901           189.582975    14.920355     1.132338   \n",
       "std           1.964154            58.126639     9.626527     0.039333   \n",
       "min           1.372222            47.883333    -6.044444     1.036111   \n",
       "25%           3.534722           142.598611     7.540278     1.106111   \n",
       "50%           4.477778           191.908333    14.213889     1.131667   \n",
       "75%           5.544444           232.043056    21.062500     1.160556   \n",
       "max          15.044444           329.544444    41.044444     1.229444   \n",
       "\n",
       "          pressure  precipitation    wind_gust    radiation   wind_speed  \\\n",
       "count  6646.000000    6646.000000  6646.000000  6646.000000  6646.000000   \n",
       "mean    935.286695       0.117364     4.872509   225.250182     3.220986   \n",
       "std       6.105083       0.245561     2.428587   266.299815     1.319704   \n",
       "min     913.800000       0.000000     1.194444     0.000000     0.855556   \n",
       "25%     932.034722       0.000000     3.284722     0.000000     2.294444   \n",
       "50%     934.761111       0.000000     4.327778    95.000000     2.961111   \n",
       "75%     937.805556       0.100000     5.627778   416.422222     3.850000   \n",
       "max     961.238889       1.700000    16.705556   954.288889     9.738889   \n",
       "\n",
       "       wind_direction        price  \n",
       "count     6646.000000  6646.000000  \n",
       "mean       189.082629    50.153506  \n",
       "std         56.712796    12.448392  \n",
       "min         50.538889     4.000000  \n",
       "25%        143.750000    43.000000  \n",
       "50%        191.208333    50.890000  \n",
       "75%        230.865278    59.950000  \n",
       "max        327.711111    85.050000  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CSV_COLUMNS = ['prediction_date', 'wind_speed_100m', \n",
    "  'wind_direction_100m', \n",
    "  'temperature', \n",
    "  'air_density',\n",
    "  'pressure', 'precipitation', 'wind_gust', 'radiation', \n",
    "  'wind_speed', 'wind_direction', 'price']\n",
    "FEATURES = CSV_COLUMNS[1:len(CSV_COLUMNS) - 1]\n",
    "LABEL = CSV_COLUMNS[0]\n",
    "\n",
    "df5 = pd.read_csv('/content/datalab/energy_forcasing/Sarah/sarah_data.csv', header = None, names = CSV_COLUMNS)\n",
    "df5.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6646"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df5.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for ii in range(10):\n",
    "#   N = 7\n",
    "#  SEQ = df5.price[(0+N*ii):(N+N*ii)]\n",
    "np.random.seed(seed=None)\n",
    "# np.random.uniform(0,6646-N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_csv(filename, start, N, J):\n",
    "  with open(filename, 'w') as ofp:\n",
    "    for ii in xrange(start, N):\n",
    "      # nn = int(round(np.random.uniform(low = 0, high = 6466-J),0))\n",
    "      seq = (df5.price[(ii):(ii+J)]/60) ## trade nn for ii to make an in seq dataset instead of random\n",
    "      line = \",\".join(map(str, seq))\n",
    "      ofp.write(line + '\\n')\n",
    "      #print(len(seq))\n",
    "      if len(seq) != J:\n",
    "        break\n",
    "\n",
    "import os\n",
    "try:\n",
    "  os.makedirs('data/CNN_W2P_model/')\n",
    "except OSError:\n",
    "  pass\n",
    "to_csv('data/CNN_W2P_model/train-1.csv', start = 0, N = 5000, J = 50)  # 1000 sequences\n",
    "to_csv('data/CNN_W2P_model/valid-1.csv', start = 5001, N = 5101, J = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.948333</td>\n",
       "      <td>0.942500</td>\n",
       "      <td>0.912333</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.781000</td>\n",
       "      <td>0.654167</td>\n",
       "      <td>0.808333</td>\n",
       "      <td>0.724833</td>\n",
       "      <td>0.992833</td>\n",
       "      <td>0.668833</td>\n",
       "      <td>...</td>\n",
       "      <td>0.658333</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.085000</td>\n",
       "      <td>0.953333</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.846000</td>\n",
       "      <td>0.784667</td>\n",
       "      <td>0.751667</td>\n",
       "      <td>0.247333</td>\n",
       "      <td>1.001667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.942500</td>\n",
       "      <td>0.912333</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.781000</td>\n",
       "      <td>0.654167</td>\n",
       "      <td>0.808333</td>\n",
       "      <td>0.724833</td>\n",
       "      <td>0.992833</td>\n",
       "      <td>0.668833</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.085000</td>\n",
       "      <td>0.953333</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.846000</td>\n",
       "      <td>0.784667</td>\n",
       "      <td>0.751667</td>\n",
       "      <td>0.247333</td>\n",
       "      <td>1.001667</td>\n",
       "      <td>1.008333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.912333</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.781000</td>\n",
       "      <td>0.654167</td>\n",
       "      <td>0.808333</td>\n",
       "      <td>0.724833</td>\n",
       "      <td>0.992833</td>\n",
       "      <td>0.668833</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.830833</td>\n",
       "      <td>...</td>\n",
       "      <td>1.085000</td>\n",
       "      <td>0.953333</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.846000</td>\n",
       "      <td>0.784667</td>\n",
       "      <td>0.751667</td>\n",
       "      <td>0.247333</td>\n",
       "      <td>1.001667</td>\n",
       "      <td>1.008333</td>\n",
       "      <td>0.790500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.781000</td>\n",
       "      <td>0.654167</td>\n",
       "      <td>0.808333</td>\n",
       "      <td>0.724833</td>\n",
       "      <td>0.992833</td>\n",
       "      <td>0.668833</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.830833</td>\n",
       "      <td>0.834500</td>\n",
       "      <td>...</td>\n",
       "      <td>0.953333</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.846000</td>\n",
       "      <td>0.784667</td>\n",
       "      <td>0.751667</td>\n",
       "      <td>0.247333</td>\n",
       "      <td>1.001667</td>\n",
       "      <td>1.008333</td>\n",
       "      <td>0.790500</td>\n",
       "      <td>1.124667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.781000</td>\n",
       "      <td>0.654167</td>\n",
       "      <td>0.808333</td>\n",
       "      <td>0.724833</td>\n",
       "      <td>0.992833</td>\n",
       "      <td>0.668833</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.830833</td>\n",
       "      <td>0.834500</td>\n",
       "      <td>0.816667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.846000</td>\n",
       "      <td>0.784667</td>\n",
       "      <td>0.751667</td>\n",
       "      <td>0.247333</td>\n",
       "      <td>1.001667</td>\n",
       "      <td>1.008333</td>\n",
       "      <td>0.790500</td>\n",
       "      <td>1.124667</td>\n",
       "      <td>0.492167</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0  0.948333  0.942500  0.912333  0.500000  0.781000  0.654167  0.808333   \n",
       "1  0.942500  0.912333  0.500000  0.781000  0.654167  0.808333  0.724833   \n",
       "2  0.912333  0.500000  0.781000  0.654167  0.808333  0.724833  0.992833   \n",
       "3  0.500000  0.781000  0.654167  0.808333  0.724833  0.992833  0.668833   \n",
       "4  0.781000  0.654167  0.808333  0.724833  0.992833  0.668833  0.750000   \n",
       "\n",
       "         7         8         9     ...           40        41        42  \\\n",
       "0  0.724833  0.992833  0.668833    ...     0.658333  0.500000  1.085000   \n",
       "1  0.992833  0.668833  0.750000    ...     0.500000  1.085000  0.953333   \n",
       "2  0.668833  0.750000  0.830833    ...     1.085000  0.953333  0.500000   \n",
       "3  0.750000  0.830833  0.834500    ...     0.953333  0.500000  0.846000   \n",
       "4  0.830833  0.834500  0.816667    ...     0.500000  0.846000  0.784667   \n",
       "\n",
       "         43        44        45        46        47        48        49  \n",
       "0  0.953333  0.500000  0.846000  0.784667  0.751667  0.247333  1.001667  \n",
       "1  0.500000  0.846000  0.784667  0.751667  0.247333  1.001667  1.008333  \n",
       "2  0.846000  0.784667  0.751667  0.247333  1.001667  1.008333  0.790500  \n",
       "3  0.784667  0.751667  0.247333  1.001667  1.008333  0.790500  1.124667  \n",
       "4  0.751667  0.247333  1.001667  1.008333  0.790500  1.124667  0.492167  \n",
       "\n",
       "[5 rows x 50 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.read_csv('data/CNN_W2P_model/train-1.csv', header = None)\n",
    "df2.head()\n",
    "#df2.describe()\n",
    "# df2.count()\n",
    "# df2.shape\n",
    "## print(df2.iloc[0], df2.iloc[1], df2.iloc[2], )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.948333</td>\n",
       "      <td>0.942500</td>\n",
       "      <td>0.912333</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.781000</td>\n",
       "      <td>0.654167</td>\n",
       "      <td>0.808333</td>\n",
       "      <td>0.724833</td>\n",
       "      <td>0.992833</td>\n",
       "      <td>0.668833</td>\n",
       "      <td>...</td>\n",
       "      <td>0.658333</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.085000</td>\n",
       "      <td>0.953333</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.846000</td>\n",
       "      <td>0.784667</td>\n",
       "      <td>0.751667</td>\n",
       "      <td>0.247333</td>\n",
       "      <td>1.001667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.942500</td>\n",
       "      <td>0.912333</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.781000</td>\n",
       "      <td>0.654167</td>\n",
       "      <td>0.808333</td>\n",
       "      <td>0.724833</td>\n",
       "      <td>0.992833</td>\n",
       "      <td>0.668833</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.085000</td>\n",
       "      <td>0.953333</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.846000</td>\n",
       "      <td>0.784667</td>\n",
       "      <td>0.751667</td>\n",
       "      <td>0.247333</td>\n",
       "      <td>1.001667</td>\n",
       "      <td>1.008333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.912333</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.781000</td>\n",
       "      <td>0.654167</td>\n",
       "      <td>0.808333</td>\n",
       "      <td>0.724833</td>\n",
       "      <td>0.992833</td>\n",
       "      <td>0.668833</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.830833</td>\n",
       "      <td>...</td>\n",
       "      <td>1.085000</td>\n",
       "      <td>0.953333</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.846000</td>\n",
       "      <td>0.784667</td>\n",
       "      <td>0.751667</td>\n",
       "      <td>0.247333</td>\n",
       "      <td>1.001667</td>\n",
       "      <td>1.008333</td>\n",
       "      <td>0.790500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.781000</td>\n",
       "      <td>0.654167</td>\n",
       "      <td>0.808333</td>\n",
       "      <td>0.724833</td>\n",
       "      <td>0.992833</td>\n",
       "      <td>0.668833</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.830833</td>\n",
       "      <td>0.834500</td>\n",
       "      <td>...</td>\n",
       "      <td>0.953333</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.846000</td>\n",
       "      <td>0.784667</td>\n",
       "      <td>0.751667</td>\n",
       "      <td>0.247333</td>\n",
       "      <td>1.001667</td>\n",
       "      <td>1.008333</td>\n",
       "      <td>0.790500</td>\n",
       "      <td>1.124667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.781000</td>\n",
       "      <td>0.654167</td>\n",
       "      <td>0.808333</td>\n",
       "      <td>0.724833</td>\n",
       "      <td>0.992833</td>\n",
       "      <td>0.668833</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.830833</td>\n",
       "      <td>0.834500</td>\n",
       "      <td>0.816667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.846000</td>\n",
       "      <td>0.784667</td>\n",
       "      <td>0.751667</td>\n",
       "      <td>0.247333</td>\n",
       "      <td>1.001667</td>\n",
       "      <td>1.008333</td>\n",
       "      <td>0.790500</td>\n",
       "      <td>1.124667</td>\n",
       "      <td>0.492167</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0  0.948333  0.942500  0.912333  0.500000  0.781000  0.654167  0.808333   \n",
       "1  0.942500  0.912333  0.500000  0.781000  0.654167  0.808333  0.724833   \n",
       "2  0.912333  0.500000  0.781000  0.654167  0.808333  0.724833  0.992833   \n",
       "3  0.500000  0.781000  0.654167  0.808333  0.724833  0.992833  0.668833   \n",
       "4  0.781000  0.654167  0.808333  0.724833  0.992833  0.668833  0.750000   \n",
       "\n",
       "         7         8         9     ...           40        41        42  \\\n",
       "0  0.724833  0.992833  0.668833    ...     0.658333  0.500000  1.085000   \n",
       "1  0.992833  0.668833  0.750000    ...     0.500000  1.085000  0.953333   \n",
       "2  0.668833  0.750000  0.830833    ...     1.085000  0.953333  0.500000   \n",
       "3  0.750000  0.830833  0.834500    ...     0.953333  0.500000  0.846000   \n",
       "4  0.830833  0.834500  0.816667    ...     0.500000  0.846000  0.784667   \n",
       "\n",
       "         43        44        45        46        47        48        49  \n",
       "0  0.953333  0.500000  0.846000  0.784667  0.751667  0.247333  1.001667  \n",
       "1  0.500000  0.846000  0.784667  0.751667  0.247333  1.001667  1.008333  \n",
       "2  0.846000  0.784667  0.751667  0.247333  1.001667  1.008333  0.790500  \n",
       "3  0.784667  0.751667  0.247333  1.001667  1.008333  0.790500  1.124667  \n",
       "4  0.751667  0.247333  1.001667  1.008333  0.790500  1.124667  0.492167  \n",
       "\n",
       "[5 rows x 50 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msk = np.random.rand(len(df2)) < 0.8\n",
    "train = df2[msk]\n",
    "test = df2[~msk]\n",
    "train\n",
    "# then normalize\n",
    "# then write to train.csv, test.csv df2.to_csv(...)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.834478</td>\n",
       "      <td>0.834212</td>\n",
       "      <td>0.828520</td>\n",
       "      <td>0.835327</td>\n",
       "      <td>0.833325</td>\n",
       "      <td>0.834935</td>\n",
       "      <td>0.829142</td>\n",
       "      <td>0.836500</td>\n",
       "      <td>0.840150</td>\n",
       "      <td>0.841215</td>\n",
       "      <td>...</td>\n",
       "      <td>0.861833</td>\n",
       "      <td>0.866928</td>\n",
       "      <td>0.866152</td>\n",
       "      <td>0.867867</td>\n",
       "      <td>0.865625</td>\n",
       "      <td>0.862798</td>\n",
       "      <td>0.859623</td>\n",
       "      <td>0.857418</td>\n",
       "      <td>0.864093</td>\n",
       "      <td>0.858880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.226580</td>\n",
       "      <td>0.226617</td>\n",
       "      <td>0.238085</td>\n",
       "      <td>0.229764</td>\n",
       "      <td>0.228138</td>\n",
       "      <td>0.230125</td>\n",
       "      <td>0.229196</td>\n",
       "      <td>0.228631</td>\n",
       "      <td>0.227486</td>\n",
       "      <td>0.227594</td>\n",
       "      <td>...</td>\n",
       "      <td>0.192697</td>\n",
       "      <td>0.191246</td>\n",
       "      <td>0.191221</td>\n",
       "      <td>0.190591</td>\n",
       "      <td>0.193574</td>\n",
       "      <td>0.193434</td>\n",
       "      <td>0.192047</td>\n",
       "      <td>0.189738</td>\n",
       "      <td>0.187701</td>\n",
       "      <td>0.186599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.208500</td>\n",
       "      <td>0.208500</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.707625</td>\n",
       "      <td>0.707625</td>\n",
       "      <td>0.707625</td>\n",
       "      <td>0.711417</td>\n",
       "      <td>0.711417</td>\n",
       "      <td>0.711417</td>\n",
       "      <td>0.707625</td>\n",
       "      <td>0.711417</td>\n",
       "      <td>0.711833</td>\n",
       "      <td>0.711833</td>\n",
       "      <td>...</td>\n",
       "      <td>0.734708</td>\n",
       "      <td>0.737333</td>\n",
       "      <td>0.737333</td>\n",
       "      <td>0.750500</td>\n",
       "      <td>0.750500</td>\n",
       "      <td>0.737333</td>\n",
       "      <td>0.737333</td>\n",
       "      <td>0.737333</td>\n",
       "      <td>0.750500</td>\n",
       "      <td>0.737333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.837750</td>\n",
       "      <td>0.837750</td>\n",
       "      <td>0.837750</td>\n",
       "      <td>0.842333</td>\n",
       "      <td>0.842333</td>\n",
       "      <td>0.842333</td>\n",
       "      <td>0.837750</td>\n",
       "      <td>0.842333</td>\n",
       "      <td>0.843333</td>\n",
       "      <td>0.846667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.867500</td>\n",
       "      <td>0.868333</td>\n",
       "      <td>0.867500</td>\n",
       "      <td>0.868333</td>\n",
       "      <td>0.868333</td>\n",
       "      <td>0.867500</td>\n",
       "      <td>0.862333</td>\n",
       "      <td>0.862333</td>\n",
       "      <td>0.867500</td>\n",
       "      <td>0.862333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.971917</td>\n",
       "      <td>0.971917</td>\n",
       "      <td>0.971917</td>\n",
       "      <td>0.971917</td>\n",
       "      <td>0.968417</td>\n",
       "      <td>0.968417</td>\n",
       "      <td>0.968333</td>\n",
       "      <td>0.968417</td>\n",
       "      <td>0.968417</td>\n",
       "      <td>0.968417</td>\n",
       "      <td>...</td>\n",
       "      <td>0.971792</td>\n",
       "      <td>0.986167</td>\n",
       "      <td>0.986167</td>\n",
       "      <td>0.986167</td>\n",
       "      <td>0.986167</td>\n",
       "      <td>0.971792</td>\n",
       "      <td>0.968375</td>\n",
       "      <td>0.968333</td>\n",
       "      <td>0.968375</td>\n",
       "      <td>0.968333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.352000</td>\n",
       "      <td>1.352000</td>\n",
       "      <td>1.352000</td>\n",
       "      <td>1.352000</td>\n",
       "      <td>1.352000</td>\n",
       "      <td>1.352000</td>\n",
       "      <td>1.352000</td>\n",
       "      <td>1.352000</td>\n",
       "      <td>1.352000</td>\n",
       "      <td>1.352000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.315000</td>\n",
       "      <td>1.315000</td>\n",
       "      <td>1.315000</td>\n",
       "      <td>1.315000</td>\n",
       "      <td>1.315000</td>\n",
       "      <td>1.315000</td>\n",
       "      <td>1.315000</td>\n",
       "      <td>1.315000</td>\n",
       "      <td>1.315000</td>\n",
       "      <td>1.315000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               0           1           2           3           4           5   \\\n",
       "count  100.000000  100.000000  100.000000  100.000000  100.000000  100.000000   \n",
       "mean     0.834478    0.834212    0.828520    0.835327    0.833325    0.834935   \n",
       "std      0.226580    0.226617    0.238085    0.229764    0.228138    0.230125   \n",
       "min      0.208500    0.208500    0.083333    0.083333    0.083333    0.083333   \n",
       "25%      0.707625    0.707625    0.707625    0.711417    0.711417    0.711417   \n",
       "50%      0.837750    0.837750    0.837750    0.842333    0.842333    0.842333   \n",
       "75%      0.971917    0.971917    0.971917    0.971917    0.968417    0.968417   \n",
       "max      1.352000    1.352000    1.352000    1.352000    1.352000    1.352000   \n",
       "\n",
       "               6           7           8           9      ...              40  \\\n",
       "count  100.000000  100.000000  100.000000  100.000000     ...      100.000000   \n",
       "mean     0.829142    0.836500    0.840150    0.841215     ...        0.861833   \n",
       "std      0.229196    0.228631    0.227486    0.227594     ...        0.192697   \n",
       "min      0.083333    0.083333    0.083333    0.083333     ...        0.083333   \n",
       "25%      0.707625    0.711417    0.711833    0.711833     ...        0.734708   \n",
       "50%      0.837750    0.842333    0.843333    0.846667     ...        0.867500   \n",
       "75%      0.968333    0.968417    0.968417    0.968417     ...        0.971792   \n",
       "max      1.352000    1.352000    1.352000    1.352000     ...        1.315000   \n",
       "\n",
       "               41          42          43          44          45          46  \\\n",
       "count  100.000000  100.000000  100.000000  100.000000  100.000000  100.000000   \n",
       "mean     0.866928    0.866152    0.867867    0.865625    0.862798    0.859623   \n",
       "std      0.191246    0.191221    0.190591    0.193574    0.193434    0.192047   \n",
       "min      0.083333    0.083333    0.083333    0.083333    0.083333    0.083333   \n",
       "25%      0.737333    0.737333    0.750500    0.750500    0.737333    0.737333   \n",
       "50%      0.868333    0.867500    0.868333    0.868333    0.867500    0.862333   \n",
       "75%      0.986167    0.986167    0.986167    0.986167    0.971792    0.968375   \n",
       "max      1.315000    1.315000    1.315000    1.315000    1.315000    1.315000   \n",
       "\n",
       "               47          48          49  \n",
       "count  100.000000  100.000000  100.000000  \n",
       "mean     0.857418    0.864093    0.858880  \n",
       "std      0.189738    0.187701    0.186599  \n",
       "min      0.083333    0.083333    0.083333  \n",
       "25%      0.737333    0.750500    0.737333  \n",
       "50%      0.862333    0.867500    0.862333  \n",
       "75%      0.968333    0.968375    0.968333  \n",
       "max      1.315000    1.315000    1.315000  \n",
       "\n",
       "[8 rows x 50 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.read_csv('data/CNN_W2P_model/valid-1.csv', header = None) #, names = CSV_COLUMNS)\n",
    "df2.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "rm -rf /content/datalab/energy_forcasing/Sarah/CNN_W2P_model/trainer\n",
    "mkdir /content/datalab/energy_forcasing/Sarah/CNN_W2P_model/trainer\n",
    "touch /content/datalab/energy_forcasing/Sarah/CNN_W2P_model/trainer/__init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'N_INPUTS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-8042a1fa1f72>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_INPUTS\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m X = tf.reshape(features,\n",
      "\u001b[0;31mNameError\u001b[0m: name 'N_INPUTS' is not defined"
     ]
    }
   ],
   "source": [
    "# read data and convert to needed format\n",
    "def read_dataset(filename, mode, batch_size=512):\n",
    "    def _input_fn():\n",
    "        def decode_csv(row):\n",
    "            # row is a string tensor containing the contents of one row\n",
    "            features = tf.decode_csv(row, record_defaults=DEFAULTS)  # string tensor -> list of 50 rank 0 float tensors\n",
    "            label = features.pop()  # remove last feature and use as label\n",
    "            features = tf.stack(features)  # list of rank 0 tensors -> single rank 1 tensor\n",
    "            return {TIMESERIES_COL: features}, label\n",
    "\n",
    "        # Create list of file names that match \"glob\" pattern (i.e. data_file_*.csv)\n",
    "        dataset = tf.data.Dataset.list_files(filename)\n",
    "        # Read in data from files\n",
    "        dataset = dataset.flat_map(tf.data.TextLineDataset)\n",
    "        # Parse text lines as comma-separated values (CSV)\n",
    "        dataset = dataset.map(decode_csv)\n",
    "\n",
    "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "            num_epochs = None  # loop indefinitely\n",
    "            dataset = dataset.shuffle(buffer_size=10 * batch_size)\n",
    "        else:\n",
    "            num_epochs = 1  # end-of-input after this\n",
    "\n",
    "        dataset = dataset.repeat(num_epochs).batch(batch_size)\n",
    "        return dataset.make_one_shot_iterator().get_next()\n",
    "\n",
    "    return _input_fn\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "\n",
    "features = tf.placeholder(tf.float32, shape=[512, N_INPUTS])\n",
    "\n",
    "X = tf.reshape(features,\n",
    "               [-1, N_INPUTS, 1])  # as a 1D \"sequence\" with only one time-series observation (height)\n",
    "c1 = tf.layers.conv1d(X, filters=N_INPUTS // 2,\n",
    "                      kernel_size=3, strides=1,\n",
    "                      padding='same', activation=tf.nn.relu)\n",
    "p1 = tf.layers.max_pooling1d(c1, pool_size=2, strides=2)\n",
    "\n",
    "c2 = tf.layers.conv1d(p1, filters=N_INPUTS // 2,\n",
    "                      kernel_size=3, strides=1,\n",
    "                      padding='same', activation=tf.nn.relu)\n",
    "p2 = tf.layers.max_pooling1d(c2, pool_size=2, strides=2)\n",
    "\n",
    "outlen = p2.shape[1] * p2.shape[2]\n",
    "c2flat = tf.reshape(p2, [-1, outlen])\n",
    "h1 = tf.layers.dense(c2flat, 3, activation=tf.nn.relu)\n",
    "predictions = tf.layers.dense(h1, 1, activation=None)  # linear output: regression\n",
    "\n",
    "N_OUTPUTS = 1\n",
    "TIMESERIES_COL = 'price'\n",
    "SEQ_LEN = 50\n",
    "\n",
    "def init(hparams):\n",
    "    global SEQ_LEN, DEFAULTS, N_INPUTS\n",
    "    SEQ_LEN = hparams['sequence_length']\n",
    "    DEFAULTS = [[0.0] for x in range(0, SEQ_LEN)]\n",
    "    N_INPUTS = SEQ_LEN - N_OUTPUTS\n",
    "    \n",
    "init({'sequence_length': 50})\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "  sess.run(init)\n",
    "  features1, labels = sess.run(read_dataset('data/CNN_W2P_model/train-1.csv', None)())\n",
    "#   features, labels = read_dataset('data/CNN_W2P_model/train-1.csv', None)()\n",
    "  preds = sess.run(predictions, feed_dict={features: features1['price']})\n",
    "# (preds*60)\n",
    "\n",
    "## Testing LSTM shapes.\n",
    "\n",
    "x = tf.reshape(features, [-1, N_INPUTS, 1])\n",
    "\n",
    "# 2. configure the RNN\n",
    "cell1 = tf.nn.rnn_cell.GRUCell(N_INPUTS * 2)\n",
    "cell2 = tf.nn.rnn_cell.GRUCell(N_INPUTS // 2)\n",
    "cells = tf.nn.rnn_cell.MultiRNNCell([cell1, cell2])\n",
    "outputs, state = tf.nn.dynamic_rnn(cells, x, dtype=tf.float32)\n",
    "# 'state' is now a tuple containing the final state of each cell layer\n",
    "# we use state[1] below to extract the final state of the final layer\n",
    "outputs = outputs[:, (N_INPUTS-1):, :] # last one only\n",
    "\n",
    "# 3. flatten lstm output and pass through a dense layer\n",
    "lstm_flat = tf.reshape(outputs, [-1, cells.output_size])\n",
    "h1 = tf.layers.dense(lstm_flat, cells.output_size//2, activation=tf.nn.relu)\n",
    "predictions_new = tf.layers.dense(h1, 1, activation=None) # (?, 1)\n",
    "\n",
    "# 3. pass rnn output through a dense layer\n",
    "h1 = tf.layers.dense(state[1], cells.output_size // 2, activation=tf.nn.relu)\n",
    "predictions = tf.layers.dense(h1, 1, activation=None)  # (?, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /content/datalab/energy_forcasing/Sarah/CNN_W2P_model/trainer/model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /content/datalab/energy_forcasing/Sarah/CNN_W2P_model/trainer/model.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "# Copyright 2017 Google Inc. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "TIMESERIES_COL = 'price'\n",
    "N_OUTPUTS = 1  # in each sequence, 1-49 are features, and 50 is label\n",
    "SEQ_LEN = None\n",
    "DEFAULTS = None\n",
    "N_INPUTS = None\n",
    "\n",
    "\n",
    "def init(hparams):\n",
    "    global SEQ_LEN, DEFAULTS, N_INPUTS\n",
    "    SEQ_LEN = hparams['sequence_length']\n",
    "    DEFAULTS = [[0.0] for x in range(0, SEQ_LEN)]\n",
    "    N_INPUTS = SEQ_LEN - N_OUTPUTS\n",
    "\n",
    "\n",
    "def linear_model(features, mode, params):\n",
    "    X = features[TIMESERIES_COL]\n",
    "    predictions = tf.layers.dense(X, 1, activation=None)\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def dnn_model(features, mode, params):\n",
    "    X = features[TIMESERIES_COL]\n",
    "    h1 = tf.layers.dense(X, 10, activation=tf.nn.relu)\n",
    "    h2 = tf.layers.dense(h1, 3, activation=tf.nn.relu)\n",
    "    predictions = tf.layers.dense(h2, 1, activation=None)  # linear output: regression\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def cnn_model(features, mode, params):\n",
    "    X = tf.reshape(features[TIMESERIES_COL],\n",
    "                   [-1, N_INPUTS, 1])  # as a 1D \"sequence\" with only one time-series observation (height)\n",
    "    c1 = tf.layers.conv1d(X, filters=N_INPUTS // 2,\n",
    "                          kernel_size=3, strides=1,\n",
    "                          padding='same', activation=tf.nn.relu)\n",
    "    p1 = tf.layers.max_pooling1d(c1, pool_size=2, strides=2)\n",
    "\n",
    "#     outlen = p1.shape[1] * p1.shape[2]\n",
    "#     c1flat = tf.reshape(p1, [-1, outlen])\n",
    "#     h1 = tf.layers.dense(c1flat, 3, activation=tf.nn.relu)\n",
    "#     predictions = tf.layers.dense(h1, 1, activation=None)  # linear output: regression\n",
    "#     return predictions\n",
    "    \n",
    "    c2 = tf.layers.conv1d(p1, filters=N_INPUTS // 2,\n",
    "                          kernel_size=3, strides=1,\n",
    "                          padding='same', activation=tf.nn.relu)\n",
    "    p2 = tf.layers.max_pooling1d(c2, pool_size=2, strides=2)\n",
    "\n",
    "    outlen = p2.shape[1] * p2.shape[2]\n",
    "    c2flat = tf.reshape(p2, [-1, outlen])\n",
    "    h1 = tf.layers.dense(c2flat, 3, activation=tf.nn.relu)\n",
    "    predictions = tf.layers.dense(h1, 1, activation=None)  # linear output: regression\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def rnn_model(features, mode, params):\n",
    "    CELL_SIZE = N_INPUTS // 3  # size of the internal state in each of the cells\n",
    "\n",
    "    # 1. dynamic_rnn needs 3D shape: [BATCH_SIZE, N_INPUTS, 1]\n",
    "    x = tf.reshape(features[TIMESERIES_COL], [-1, N_INPUTS, 1])\n",
    "\n",
    "    # 2. configure the RNN\n",
    "    cell = tf.nn.rnn_cell.GRUCell(CELL_SIZE)\n",
    "    outputs, state = tf.nn.dynamic_rnn(cell, x, dtype=tf.float32)\n",
    "\n",
    "    # 3. pass rnn output through a dense layer\n",
    "    h1 = tf.layers.dense(state, N_INPUTS // 2, activation=tf.nn.relu)\n",
    "    h2 = tf.layers.dense(h1, N_INPUTS // 2, activation=tf.nn.relu6)\n",
    "    h3 = tf.layers.dense(h2, N_INPUTS // 2, activation=tf.nn.relu)\n",
    "    \n",
    "    predictions = tf.layers.dense(h3, 1, activation=None)  # (?, 1)\n",
    "    return predictions\n",
    "\n",
    "\n",
    "# 2-layer RNN\n",
    "def rnn2_model(features, mode, params):\n",
    "    # dynamic_rnn needs 3D shape: [BATCH_SIZE, N_INPUTS, 1]\n",
    "    x = tf.reshape(features[TIMESERIES_COL], [-1, N_INPUTS, 1])\n",
    "\n",
    "    # 2. configure the RNN\n",
    "    cell1 = tf.nn.rnn_cell.GRUCell(N_INPUTS * 2)\n",
    "    cell2 = tf.nn.rnn_cell.GRUCell(N_INPUTS // 2)\n",
    "    cells = tf.nn.rnn_cell.MultiRNNCell([cell1, cell2])\n",
    "    outputs, state = tf.nn.dynamic_rnn(cells, x, dtype=tf.float32)\n",
    "    # 'state' is now a tuple containing the final state of each cell layer\n",
    "    # we use state[1] below to extract the final state of the final layer\n",
    "    outputs = outputs[:, (N_INPUTS-1):, :] # last one only\n",
    "\n",
    "    # 3. flatten lstm output and pass through a dense layer\n",
    "    lstm_flat = tf.reshape(outputs, [-1, cells.output_size])\n",
    "    h1 = tf.layers.dense(lstm_flat, cells.output_size//2, activation=tf.nn.relu)\n",
    "    predictions = tf.layers.dense(h1, 1, activation=None) # (?, 1)\n",
    "    return predictions\n",
    "  \n",
    "    # 3. pass rnn output through a dense layer\n",
    "    h1 = tf.layers.dense(state[1], cells.output_size // 2, activation=tf.nn.relu)\n",
    "    predictions = tf.layers.dense(h1, 1, activation=None)  # (?, 1)\n",
    "    return predictions\n",
    "\n",
    "\n",
    "# create N-1 predictions\n",
    "def rnnN_model(features, mode, params):\n",
    "    # dynamic_rnn needs 3D shape: [BATCH_SIZE, N_INPUTS, 1]\n",
    "    x = tf.reshape(features[TIMESERIES_COL], [-1, N_INPUTS, 1])\n",
    "\n",
    "    # 2. configure the RNN\n",
    "    cell1 = tf.nn.rnn_cell.GRUCell(N_INPUTS * 2)\n",
    "    cell2 = tf.nn.rnn_cell.GRUCell(N_INPUTS // 2)\n",
    "    cells = tf.nn.rnn_cell.MultiRNNCell([cell1, cell2])\n",
    "    outputs, state = tf.nn.dynamic_rnn(cells, x, dtype=tf.float32)\n",
    "    # 'outputs' contains the state of the final layer for every time step\n",
    "    # not just the last time step (?,N_INPUTS, final cell size)\n",
    "    \n",
    "    # 3. pass state for each time step through a DNN, to get a prediction\n",
    "    # for each time step \n",
    "    h1 = tf.layers.dense(outputs, cells.output_size, activation=tf.nn.relu)\n",
    "    h2 = tf.layers.dense(h1, cells.output_size // 2, activation=tf.nn.relu)\n",
    "    predictions = tf.layers.dense(h2, 1, activation=None)  # (?, N_INPUTS, 1)\n",
    "    predictions = tf.reshape(predictions, [-1, N_INPUTS])\n",
    "    return predictions # return prediction for each time step\n",
    "\n",
    "\n",
    "# read data and convert to needed format\n",
    "def read_dataset(filename, mode, batch_size=512):\n",
    "    def _input_fn():\n",
    "        def decode_csv(row):\n",
    "            # row is a string tensor containing the contents of one row\n",
    "            features = tf.decode_csv(row, record_defaults=DEFAULTS)  # string tensor -> list of 50 rank 0 float tensors\n",
    "            label = features.pop()  # remove last feature and use as label\n",
    "            features = tf.stack(features)  # list of rank 0 tensors -> single rank 1 tensor\n",
    "            return {TIMESERIES_COL: features}, label\n",
    "\n",
    "        # Create list of file names that match \"glob\" pattern (i.e. data_file_*.csv)\n",
    "        dataset = tf.data.Dataset.list_files(filename)\n",
    "        # Read in data from files\n",
    "        dataset = dataset.flat_map(tf.data.TextLineDataset)\n",
    "        # Parse text lines as comma-separated values (CSV)\n",
    "        dataset = dataset.map(decode_csv)\n",
    "\n",
    "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "            num_epochs = None  # loop indefinitely\n",
    "            dataset = dataset.shuffle(buffer_size=10 * batch_size)\n",
    "        else:\n",
    "            num_epochs = 1  # end-of-input after this\n",
    "\n",
    "        dataset = dataset.repeat(num_epochs).batch(batch_size)\n",
    "        return dataset.make_one_shot_iterator().get_next()\n",
    "\n",
    "    return _input_fn\n",
    "\n",
    "\n",
    "def serving_input_fn():\n",
    "    feature_placeholders = {\n",
    "        TIMESERIES_COL: tf.placeholder(tf.float32, [None, N_INPUTS])\n",
    "    }\n",
    "\n",
    "    features = {\n",
    "        key: tf.expand_dims(tensor, -1)\n",
    "        for key, tensor in feature_placeholders.items()\n",
    "    }\n",
    "    features[TIMESERIES_COL] = tf.squeeze(features[TIMESERIES_COL], axis=[2])\n",
    "\n",
    "    return tf.estimator.export.ServingInputReceiver(features, feature_placeholders)\n",
    "\n",
    "\n",
    "def compute_errors(features, labels, predictions):\n",
    "    labels = tf.expand_dims(labels, -1)  # rank 1 -> rank 2 to match rank of predictions\n",
    "\n",
    "    if predictions.shape[1] == 1:\n",
    "        loss = tf.losses.mean_squared_error(labels, predictions)\n",
    "        rmse = tf.metrics.root_mean_squared_error(labels, predictions)\n",
    "        return loss, rmse\n",
    "    else:\n",
    "        # one prediction for every input in sequence\n",
    "        # get 1-N of (x + label)\n",
    "        labelsN = tf.concat([features[TIMESERIES_COL], labels], axis=1)\n",
    "        labelsN = labelsN[:, 1:]\n",
    "        # loss is computed from the last 1/3 of the series\n",
    "        N = (2 * N_INPUTS) // 3\n",
    "        loss = tf.losses.mean_squared_error(labelsN[:, N:], predictions[:, N:])\n",
    "        # rmse is computed from last prediction and last label\n",
    "        lastPred = predictions[:, -1]\n",
    "        rmse = tf.metrics.root_mean_squared_error(labels, lastPred)\n",
    "        return loss, rmse\n",
    "\n",
    "# RMSE when predicting same as last value\n",
    "def same_as_last_benchmark(features, labels):\n",
    "    predictions = features[TIMESERIES_COL][:,-1] # last value in input sequence\n",
    "    return tf.metrics.root_mean_squared_error(labels, predictions)\n",
    "\n",
    "\n",
    "# create the inference model\n",
    "def sequence_regressor(features, labels, mode, params):\n",
    "    # 1. run the appropriate model\n",
    "    model_functions = {\n",
    "        'linear': linear_model,\n",
    "        'dnn': dnn_model,\n",
    "        'cnn': cnn_model,\n",
    "        'rnn': rnn_model,\n",
    "        'rnn2': rnn2_model,\n",
    "        'rnnN': rnnN_model}\n",
    "    model_function = model_functions[params['model']]\n",
    "    print(model_function)\n",
    "    predictions = model_function(features, mode, params)\n",
    "\n",
    "    # 2. loss function, training/eval ops\n",
    "    loss = None\n",
    "    train_op = None\n",
    "    eval_metric_ops = None\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN or mode == tf.estimator.ModeKeys.EVAL:\n",
    "        loss, rmse = compute_errors(features, labels, predictions)\n",
    "    \n",
    "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "            # this is needed for batch normalization, but has no effect otherwise\n",
    "            update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "            with tf.control_dependencies(update_ops):\n",
    "                # 2b. set up training operation\n",
    "                train_op = tf.contrib.layers.optimize_loss(\n",
    "                    loss,\n",
    "                    tf.train.get_global_step(),\n",
    "                    learning_rate=params['learning_rate'],\n",
    "                    optimizer=\"Adam\")\n",
    "\n",
    "        # 2c. eval metric\n",
    "        eval_metric_ops = {\n",
    "            \"RMSE\": rmse,\n",
    "            \"RMSE_same_as_last\": same_as_last_benchmark(features, labels),\n",
    "        }\n",
    "\n",
    "    # 3. Create predictions\n",
    "    if predictions.shape[1] != 1:\n",
    "        predictions = predictions[:, -1]  # last predicted value\n",
    "    predictions_dict = {\"predicted\": predictions}\n",
    "\n",
    "    # 4. return EstimatorSpec\n",
    "    return tf.estimator.EstimatorSpec(\n",
    "        mode=mode,\n",
    "        predictions=predictions_dict,\n",
    "        loss=loss,\n",
    "        train_op=train_op,\n",
    "        eval_metric_ops=eval_metric_ops,\n",
    "        export_outputs={\n",
    "            'predictions': tf.estimator.export.PredictOutput(predictions_dict)}\n",
    "    )\n",
    "\n",
    "\n",
    "def train_and_evaluate(output_dir, hparams):\n",
    "    get_train = read_dataset(hparams['train_data_path'],\n",
    "                             tf.estimator.ModeKeys.TRAIN,\n",
    "                             hparams['train_batch_size'])\n",
    "    get_valid = read_dataset(hparams['eval_data_path'],\n",
    "                             tf.estimator.ModeKeys.EVAL,\n",
    "                             1000)\n",
    "    estimator = tf.estimator.Estimator(model_fn=sequence_regressor,\n",
    "                                       params=hparams,\n",
    "                                       config=tf.estimator.RunConfig(\n",
    "                                           save_checkpoints_steps=50,\n",
    "                                         save_summary_steps=50\n",
    "                                       ),\n",
    "                                       model_dir=output_dir)\n",
    "    train_spec = tf.estimator.TrainSpec(input_fn=get_train,\n",
    "                                        max_steps=hparams['train_steps'])\n",
    "    exporter = tf.estimator.LatestExporter('exporter', serving_input_fn)\n",
    "    eval_spec = tf.estimator.EvalSpec(input_fn=get_valid,\n",
    "                                      steps=None,\n",
    "                                      exporters=exporter,\n",
    "                                      #start_delay_secs=hparams['eval_delay_secs'],\n",
    "                                      #throttle_secs=hparams['min_eval_frequency']\n",
    "                                     )\n",
    "    tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /content/datalab/energy_forcasing/Sarah/CNN_W2P_model/trainer/task.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /content/datalab/energy_forcasing/Sarah/CNN_W2P_model/trainer/task.py\n",
    "\n",
    "# Copyright 2017 Google Inc. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "\"\"\"Example implementation of code to run on the Cloud ML service.\n",
    "\"\"\"\n",
    "\n",
    "import traceback\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "from . import model\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  parser = argparse.ArgumentParser()\n",
    "  # Input Arguments\n",
    "  parser.add_argument(\n",
    "      '--train_data_path',\n",
    "      help='GCS or local path to training data',\n",
    "      required=True\n",
    "  )\n",
    "  parser.add_argument(\n",
    "      '--eval_data_path',\n",
    "      help='GCS or local path to evaluation data',\n",
    "      required=True\n",
    "  )\n",
    "  parser.add_argument(\n",
    "      '--train_batch_size',\n",
    "      help='Batch size for training steps',\n",
    "      type=int,\n",
    "      default=100\n",
    "  )\n",
    "  parser.add_argument(\n",
    "      '--learning_rate',\n",
    "      help='Initial learning rate for training',\n",
    "      type=float,\n",
    "      default=0.01\n",
    "  )\n",
    "  parser.add_argument(\n",
    "      '--train_steps',\n",
    "      help=\"\"\"\\\n",
    "      Steps to run the training job for. A step is one batch-size,\\\n",
    "      \"\"\",\n",
    "      type=int,\n",
    "      default=0\n",
    "  )\n",
    "  parser.add_argument(\n",
    "      '--sequence_length',\n",
    "      help=\"\"\"\\\n",
    "      This model works with fixed length sequences. 1-(N-1) are inputs, last is output\n",
    "      \"\"\",\n",
    "      type=int,\n",
    "      default=10\n",
    "  )\n",
    "  parser.add_argument(\n",
    "      '--output_dir',\n",
    "      help='GCS location to write checkpoints and export models',\n",
    "      required=True\n",
    "  )\n",
    "  model_names = [name.replace('_model','') \\\n",
    "                   for name in dir(model) \\\n",
    "                     if name.endswith('_model')]\n",
    "  parser.add_argument(\n",
    "      '--model',\n",
    "      help='Type of model. Supported types are {}'.format(model_names),\n",
    "      required=True\n",
    "  )\n",
    "  parser.add_argument(\n",
    "      '--job-dir',\n",
    "      help='this model ignores this field, but it is required by gcloud',\n",
    "      default='junk'\n",
    "  )\n",
    "  parser.add_argument(\n",
    "      '--eval_delay_secs',\n",
    "      help='How long to wait before running first evaluation',\n",
    "      default=10,\n",
    "      type=int\n",
    "  )\n",
    "  parser.add_argument(\n",
    "      '--min_eval_frequency',\n",
    "      help='Minimum number of training steps between evaluations',\n",
    "      default=60,\n",
    "      type=int\n",
    "  )\n",
    "\n",
    "  args = parser.parse_args()\n",
    "  hparams = args.__dict__\n",
    "  \n",
    "  # unused args provided by service\n",
    "  hparams.pop('job_dir', None)\n",
    "  hparams.pop('job-dir', None)\n",
    "\n",
    "  output_dir = hparams.pop('output_dir')\n",
    "\n",
    "  # Append trial_id to path if we are doing hptuning\n",
    "  # This code can be removed if you are not using hyperparameter tuning\n",
    "  output_dir = os.path.join(\n",
    "      output_dir,\n",
    "      json.loads(\n",
    "          os.environ.get('TF_CONFIG', '{}')\n",
    "      ).get('task', {}).get('trial', '')\n",
    "  )\n",
    "\n",
    "  # calculate train_steps if not provided\n",
    "  if hparams['train_steps'] < 1:\n",
    "     # 1,000 steps at batch_size of 100\n",
    "     hparams['train_steps'] = (1000 * 100) // hparams['train_batch_size']\n",
    "     print (\"Training for {} steps\".format(hparams['train_steps']))\n",
    "\n",
    "  model.init(hparams)\n",
    "\n",
    "  # Run the training job\n",
    "  model.train_and_evaluate(output_dir, hparams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/datalab/energy_forcasing/Sarah\n",
      "__init__.py  model.py  task.py\n"
     ]
    }
   ],
   "source": [
    "! pwd\n",
    "#! ls\n",
    "#!{PWD}/CNN_W2P_model/trainer\n",
    "\n",
    "! ls /content/datalab/energy_forcasing/Sarah/CNN_W2P_model/trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run the model locally on the vm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function dnn_model at 0x7fe506f37320>\n",
      "<function dnn_model at 0x7fe506f37320>\n",
      "<function dnn_model at 0x7fe506f37320>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/envs/py2env/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "INFO:tensorflow:TF_CONFIG environment variable: {u'environment': u'cloud', u'cluster': {}, u'job': {u'args': [u'--train_data_path=/content/datalab/energy_forcasing/Sarah/data/CNN_W2P_model/train-1.csv', u'--eval_data_path=/content/datalab/energy_forcasing/Sarah/data/CNN_W2P_model/valid-1.csv', u'--output_dir=/content/datalab/energy_forcasing/Sarah/CNN_W2P_model/trained', u'--model=dnn', u'--train_steps=2000', u'--sequence_length=50'], u'job_name': u'trainer.task'}, u'task': {}}\n",
      "INFO:tensorflow:Using config: {'_save_checkpoints_secs': None, '_session_config': None, '_keep_checkpoint_max': 5, '_task_type': 'worker', '_train_distribute': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fe506f2bd50>, '_evaluation_master': '', '_save_checkpoints_steps': 50, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_master': '', '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_model_dir': '/content/datalab/energy_forcasing/Sarah/CNN_W2P_model/trained/', '_global_id_in_cluster': 0, '_save_summary_steps': 50}\n",
      "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n",
      "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after 600 secs (eval_spec.throttle_secs) or training is finished.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "2018-09-21 18:53:06.355297: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /content/datalab/energy_forcasing/Sarah/CNN_W2P_model/trained/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.7171364, step = 1\n",
      "INFO:tensorflow:Saving checkpoints for 51 into /content/datalab/energy_forcasing/Sarah/CNN_W2P_model/trained/model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 101 into /content/datalab/energy_forcasing/Sarah/CNN_W2P_model/trained/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 45.2996\n",
      "INFO:tensorflow:loss = 0.042957857, step = 101 (2.208 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 151 into /content/datalab/energy_forcasing/Sarah/CNN_W2P_model/trained/model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 201 into /content/datalab/energy_forcasing/Sarah/CNN_W2P_model/trained/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 47.6524\n",
      "INFO:tensorflow:loss = 0.043099426, step = 201 (2.098 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 251 into /content/datalab/energy_forcasing/Sarah/CNN_W2P_model/trained/model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 301 into /content/datalab/energy_forcasing/Sarah/CNN_W2P_model/trained/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 46.6496\n",
      "INFO:tensorflow:loss = 0.03605667, step = 301 (2.144 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 351 into /content/datalab/energy_forcasing/Sarah/CNN_W2P_model/trained/model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 401 into /content/datalab/energy_forcasing/Sarah/CNN_W2P_model/trained/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 47.5453\n",
      "INFO:tensorflow:loss = 0.04115568, step = 401 (2.103 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 451 into /content/datalab/energy_forcasing/Sarah/CNN_W2P_model/trained/model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 501 into /content/datalab/energy_forcasing/Sarah/CNN_W2P_model/trained/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 49.394\n",
      "INFO:tensorflow:loss = 0.03628118, step = 501 (2.025 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 551 into /content/datalab/energy_forcasing/Sarah/CNN_W2P_model/trained/model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 601 into /content/datalab/energy_forcasing/Sarah/CNN_W2P_model/trained/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 47.5641\n",
      "INFO:tensorflow:loss = 0.03700811, step = 601 (2.102 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 651 into /content/datalab/energy_forcasing/Sarah/CNN_W2P_model/trained/model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 701 into /content/datalab/energy_forcasing/Sarah/CNN_W2P_model/trained/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 48.3778\n",
      "INFO:tensorflow:loss = 0.03023376, step = 701 (2.067 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 751 into /content/datalab/energy_forcasing/Sarah/CNN_W2P_model/trained/model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 801 into /content/datalab/energy_forcasing/Sarah/CNN_W2P_model/trained/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 47.8198\n",
      "INFO:tensorflow:loss = 0.035335235, step = 801 (2.091 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 851 into /content/datalab/energy_forcasing/Sarah/CNN_W2P_model/trained/model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 901 into /content/datalab/energy_forcasing/Sarah/CNN_W2P_model/trained/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 48.0989\n",
      "INFO:tensorflow:loss = 0.035935715, step = 901 (2.079 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 951 into /content/datalab/energy_forcasing/Sarah/CNN_W2P_model/trained/model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 1001 into /content/datalab/energy_forcasing/Sarah/CNN_W2P_model/trained/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 47.3065\n",
      "INFO:tensorflow:loss = 0.04348758, step = 1001 (2.114 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1051 into /content/datalab/energy_forcasing/Sarah/CNN_W2P_model/trained/model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 1101 into /content/datalab/energy_forcasing/Sarah/CNN_W2P_model/trained/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 44.0828\n",
      "INFO:tensorflow:loss = 0.03601997, step = 1101 (2.269 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1151 into /content/datalab/energy_forcasing/Sarah/CNN_W2P_model/trained/model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 1201 into /content/datalab/energy_forcasing/Sarah/CNN_W2P_model/trained/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 48.4331\n",
      "INFO:tensorflow:loss = 0.04866679, step = 1201 (2.065 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1251 into /content/datalab/energy_forcasing/Sarah/CNN_W2P_model/trained/model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 1301 into /content/datalab/energy_forcasing/Sarah/CNN_W2P_model/trained/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 49.0527\n",
      "INFO:tensorflow:loss = 0.042962976, step = 1301 (2.039 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1351 into /content/datalab/energy_forcasing/Sarah/CNN_W2P_model/trained/model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 1401 into /content/datalab/energy_forcasing/Sarah/CNN_W2P_model/trained/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 48.1409\n",
      "INFO:tensorflow:loss = 0.04128041, step = 1401 (2.077 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1451 into /content/datalab/energy_forcasing/Sarah/CNN_W2P_model/trained/model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 1501 into /content/datalab/energy_forcasing/Sarah/CNN_W2P_model/trained/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 49.4431\n",
      "INFO:tensorflow:loss = 0.037488624, step = 1501 (2.022 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1551 into /content/datalab/energy_forcasing/Sarah/CNN_W2P_model/trained/model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 1601 into /content/datalab/energy_forcasing/Sarah/CNN_W2P_model/trained/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 47.9818\n",
      "INFO:tensorflow:loss = 0.038493972, step = 1601 (2.084 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1651 into /content/datalab/energy_forcasing/Sarah/CNN_W2P_model/trained/model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 1701 into /content/datalab/energy_forcasing/Sarah/CNN_W2P_model/trained/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 47.485\n",
      "INFO:tensorflow:loss = 0.04123708, step = 1701 (2.106 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1751 into /content/datalab/energy_forcasing/Sarah/CNN_W2P_model/trained/model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 1801 into /content/datalab/energy_forcasing/Sarah/CNN_W2P_model/trained/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 49.1781\n",
      "INFO:tensorflow:loss = 0.044216257, step = 1801 (2.033 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1851 into /content/datalab/energy_forcasing/Sarah/CNN_W2P_model/trained/model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 1901 into /content/datalab/energy_forcasing/Sarah/CNN_W2P_model/trained/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 48.9551\n",
      "INFO:tensorflow:loss = 0.042248074, step = 1901 (2.043 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1951 into /content/datalab/energy_forcasing/Sarah/CNN_W2P_model/trained/model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 2000 into /content/datalab/energy_forcasing/Sarah/CNN_W2P_model/trained/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.039614484.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-09-21-18:53:48\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /content/datalab/energy_forcasing/Sarah/CNN_W2P_model/trained/model.ckpt-2000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-09-21-18:53:48\n",
      "INFO:tensorflow:Saving dict for global step 2000: RMSE = 0.18593505, RMSE_same_as_last = 0.2641506, global_step = 2000, loss = 0.03457184\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['serving_default', 'predictions']\n",
      "INFO:tensorflow:Restoring parameters from /content/datalab/energy_forcasing/Sarah/CNN_W2P_model/trained/model.ckpt-2000\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: /content/datalab/energy_forcasing/Sarah/CNN_W2P_model/trained/export/exporter/temp-1537556029/saved_model.pb\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "DATADIR=$(pwd)/data/CNN_W2P_model \n",
    "OUTDIR=$(pwd)/CNN_W2P_model/trained\n",
    "SEQ_LEN=50\n",
    "rm -rf $OUTDIR\n",
    "gcloud ml-engine local train \\\n",
    "   --module-name=trainer.task \\\n",
    "   --package-path=${PWD}/CNN_W2P_model/trainer \\\n",
    "   -- \\\n",
    "   --train_data_path=\"${DATADIR}/train-1.csv\" \\\n",
    "   --eval_data_path=\"${DATADIR}/valid-1.csv\"  \\\n",
    "   --output_dir=${OUTDIR} \\\n",
    "   --model=dnn --train_steps=2000  --sequence_length=$SEQ_LEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34m1537556029\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "ls CNN_W2P_model/trained/export/exporter/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile test.json\n",
    "[45.52, 45.24, 43.792, 24.0, 37.488, 31.400000000000002, 38.800000000000004, 34.792, 47.656000000000006, 32.104000000000006, 36.0, 39.88, 40.056000000000004, 39.2, 34.952, 50.42400000000001, 36.472, 42.848000000000006, 30.496, 44.712, 32.128, 52.08, 31.664, 47.88, 44.0, 51.352000000000004, 42.624, 54.879999999999995, 32.800000000000004, 31.400000000000002, 30.680000000000003, 44.608000000000004, 32.256, 50.072, 37.424, 39.88, 47.752, 44.800000000000004, 52.08, 46.952, 31.6, 24.0, 52.08, 45.760000000000005, 24.0, 40.608000000000004, 37.664, 36.080000000000005, 11.872]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICTED\n",
      "[6.672857761383057]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: /usr/local/envs/py2env/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "2018-09-21 18:54:24.660602: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "model=$(ls CNN_W2P_model/trained/export/exporter | tail -1)\n",
    "gcloud ml-engine local predict \\\n",
    "    --model-dir=CNN_W2P_model/trained/export/exporter/$model \\\n",
    "    --json-instances=./test.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile test.json\n",
    "{\"price\": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICTED\n",
      "[0.5992636680603027]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: /usr/local/envs/py2env/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "2018-09-21 18:54:27.382280: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "model=$(ls CNN_W2P_model/trained/export/exporter | tail -1)\n",
    "gcloud ml-engine local predict \\\n",
    "    --model-dir=CNN_W2P_model/trained/export/exporter/$model \\\n",
    "    --json-instances=./test.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p>TensorBoard was started successfully with pid 28794. Click <a href=\"/_proxy/59269/\" target=\"_blank\">here</a> to access it.</p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "28794"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from google.datalab.ml import TensorBoard\n",
    "TensorBoard().start('CNN_W2P_model'.format(BUCKET))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make training and test data for the cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.rmtree('/data/CNN_W2P_model ', ignore_errors=True)\n",
    "#os.makedirs('/data/CNN_W2P_model/')\n",
    "\n",
    "#for i in xrange(0,3):\n",
    "#  to_csv('data/CNN_W2P_model/train-{}.csv'.format(i), N = 1000, J = 50)  # 1000 sequences\n",
    "#  to_csv('data/CNN_W2P_model/valid-{}.csv'.format(i), N = 100, J = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%bash\n",
    "gsutil -m rm -rf gs://${BUCKET}/CNN_W2P_model/*\n",
    "gsutil -m cp data/CNN_W2P_model/*.csv gs://${BUCKET}/CNN_W2P_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Push out to the cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN_W2P_model_dnn180921_185544\n",
      "jobId: CNN_W2P_model_dnn180921_185544\n",
      "state: QUEUED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CommandException: 1 files/objects could not be removed.\n",
      "Job [CNN_W2P_model_dnn180921_185544] submitted successfully.\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ml-engine jobs describe CNN_W2P_model_dnn180921_185544\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ml-engine jobs stream-logs CNN_W2P_model_dnn180921_185544\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "for MODEL in dnn ; do ## rnn rnn2 rnnN; do ## linear dnn \n",
    "  OUTDIR=gs://${BUCKET}/CNN_W2P_model/${MODEL}\n",
    "  JOBNAME=CNN_W2P_model_${MODEL}$(date -u +%y%m%d_%H%M%S)\n",
    "  SEQ_LEN=50\n",
    "  REGION=us-central1\n",
    "  gsutil -m rm -rf $OUTDIR\n",
    "  echo $JOBNAME\n",
    "  gcloud ml-engine jobs submit training $JOBNAME \\\n",
    "     --region=$REGION \\\n",
    "     --module-name=trainer.task \\\n",
    "     --package-path=${PWD}/CNN_W2P_model/trainer \\\n",
    "     --job-dir=$OUTDIR \\\n",
    "     --scale-tier=PREMIUM_1 \\\n",
    "     --runtime-version=$TFVERSION \\\n",
    "     -- \\\n",
    "     --train_data_path=\"gs://${BUCKET}/CNN_W2P_model/train-1.csv\" \\\n",
    "     --eval_data_path=\"gs://${BUCKET}/CNN_W2P_model/valid-1.csv\"  \\\n",
    "     --output_dir=$OUTDIR \\\n",
    "     --train_steps=500 --sequence_length=$SEQ_LEN --model=$MODEL\n",
    "done\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p>TensorBoard was started successfully with pid 31033. Click <a href=\"/_proxy/35585/\" target=\"_blank\">here</a> to access it.</p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "31033"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from google.datalab.ml import TensorBoard\n",
    "TensorBoard().start('gs://{}/CNN_W2P_model'.format(BUCKET))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopped TensorBoard with pid 28744\n",
      "Stopped TensorBoard with pid 28794\n",
      "Stopped TensorBoard with pid 29716\n"
     ]
    }
   ],
   "source": [
    "for pid in TensorBoard.list()['pid']:\n",
    "  TensorBoard().stop(pid)\n",
    "  print 'Stopped TensorBoard with pid {}'.format(pid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## command line access of the TB\n",
    "print('tensorboard --logdir=gs://{}/CNN_W2P_model --port=8083'.format(BUCKET))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile  hyperparam.yaml\n",
    "trainingInput:\n",
    "  hyperparameters:\n",
    "    goal: MINIMIZE\n",
    "    maxTrials: 5\n",
    "    maxParallelTrials: 1\n",
    "    hyperparameterMetricTag: rmse\n",
    "    params:\n",
    "    - parameterName: learning_rate\n",
    "      type: DOUBLE\n",
    "      minValue: 0.01\n",
    "      maxValue: 0.1\n",
    "      scaleType: UNIT_LOG_SCALE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "MODEL=dnn\n",
    "OUTDIR=gs://${BUCKET}/CNN_W2P_model/${MODEL}\n",
    "JOBNAME=CNN_W2P_model_${MODEL}$(date -u +%y%m%d_%H%M%S)\n",
    "\n",
    "SEQ_LEN=50\n",
    "REGION=us-central1\n",
    "\n",
    "#gsutil -m rm -rf $OUTDIR\n",
    "echo $JOBNAME\n",
    "\n",
    "gcloud ml-engine jobs submit training $JOBNAME \\\n",
    "     --region=$REGION \\\n",
    "     --module-name=trainer.task \\\n",
    "     --package-path=${PWD}/CNN_W2P_model/trainer \\\n",
    "     --job-dir=$OUTDIR \\\n",
    "     --scale-tier=PREMIUM_1 \\\n",
    "     --runtime-version=$TFVERSION \\\n",
    "     --config=hyperparam.yaml \\\n",
    "     -- \\\n",
    "     --train_data_path=\"gs://${BUCKET}/CNN_W2P_model/train-1.csv\" \\\n",
    "     --eval_data_path=\"gs://${BUCKET}/CNN_W2P_model/valid-1.csv\"  \\\n",
    "     --output_dir=$OUTDIR \\\n",
    "     --train_steps=5000 --sequence_length=$SEQ_LEN --model=$MODEL\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud ml-engine jobs describe CNN_W2P_model_dnn180921_165257"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the data into something that can be read into the model, so strings that are 49 long where the 50th price is perdicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://sarah-bucket/CNN_W2P_model/dnn/export/exporter/\n",
      "gs://sarah-bucket/CNN_W2P_model/dnn/export/exporter/1537556449/\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "gsutil ls gs://${BUCKET}/CNN_W2P_model/dnn/export/exporter/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting and deploying CNN_W2P_model ml_on_gcp from gs://sarah-bucket/CNN_W2P_model/dnn/export/exporter/1537556449/ ... this will take a few minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created ml engine model [projects/qwiklabs-gcp-aebfb78fe0f1b1d1/models/CNN_W2P_model].\n",
      "Creating version (this might take a few minutes)......\n",
      "............................................................................................done.\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "MODEL_NAME=\"CNN_W2P_model\"\n",
    "MODEL_VERSION=\"ml_on_gcp\"\n",
    "MODEL_LOCATION=$(gsutil ls gs://${BUCKET}/CNN_W2P_model/dnn/export/exporter/ | tail -1)\n",
    "echo \"Deleting and deploying $MODEL_NAME $MODEL_VERSION from $MODEL_LOCATION ... this will take a few minutes\"\n",
    "#gcloud ml-engine versions delete ${MODEL_VERSION} --model ${MODEL_NAME}\n",
    "#gcloud ml-engine models delete ${MODEL_NAME}\n",
    "gcloud ml-engine models create ${MODEL_NAME} --regions $REGION\n",
    "gcloud ml-engine versions create ${MODEL_VERSION} --model ${MODEL_NAME} --origin ${MODEL_LOCATION} --runtime-version $TFVERSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make data to feed to model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_model_csv(filename, N, J):\n",
    "  with open(filename, 'w') as ofp:\n",
    "    N = df5.shape[0] - J\n",
    "    for ii in xrange(0, N):\n",
    "      seq = df5.price[(0+ii):(J+ii)]\n",
    "      line = \",\".join(map(str, seq))\n",
    "      ofp.write(line + '\\n')\n",
    "      #print(len(seq))\n",
    "      if len(seq) != J:\n",
    "        break\n",
    "\n",
    "import os\n",
    "try:\n",
    "  os.makedirs('data/CNN_W2P_model/')\n",
    "except OSError:\n",
    "  pass\n",
    "\n",
    "to_model_csv('data/CNN_W2P_model/to_model.csv', N = 1000, J = 49) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6597, 49)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_data = pd.read_csv('/content/datalab/energy_forcasing/Sarah/data/CNN_W2P_model/to_model.csv', header = None)\n",
    "model_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from oauth2client.client import GoogleCredentials\n",
    "import requests\n",
    "import json\n",
    "\n",
    "MODEL_NAME=\"CNN_W2P_model_dnn\"\n",
    "MODEL_VERSION=\"ml_on_gcp\"\n",
    "\n",
    "token = GoogleCredentials.get_application_default().get_access_token().access_token\n",
    "api = 'https://ml.googleapis.com/v1/projects/{}/models/{}/versions/{}:predict' \\\n",
    "         .format(PROJECT, MODEL_NAME, MODEL_VERSION)\n",
    "headers = {'Authorization': 'Bearer ' + token }\n",
    "data = {\n",
    "  'instances': [\n",
    "    {\n",
    "      'price': prices\n",
    "    },\n",
    "  ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 55.11,62.0,46.79,44.0,44.0,44.86,60.89,58.69,36.49,57.69,51.2,48.79,48.35,42.2,46.37,62.5,62.5,54.0,52.54,62.18,48.5,55.35,52.0,40.97,42.6,64.1,61.59,48.9,37.4,45.93,37.3,65.48,62.5,65.03,57.07,23.35,54.56,28.02,39.84,51.0,42.46,47.02,37.1,38.0,38.0,60.99,50.01,19.52,39.98,47.96"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'error': {u'status': u'NOT_FOUND', u'message': u'Field: name Error: The model resource: \"CNN_W2P_model_dnn\" was not found. Please create the Cloud ML model resource first by using \\'gcloud ml-engine models create CNN_W2P_model_dnn\\'.', u'code': 404, u'details': [{u'fieldViolations': [{u'field': u'name', u'description': u'The model resource: \"CNN_W2P_model_dnn\" was not found. Please create the Cloud ML model resource first by using \\'gcloud ml-engine models create CNN_W2P_model_dnn\\'.'}], u'@type': u'type.googleapis.com/google.rpc.BadRequest'}]}}\n",
      "{u'error': {u'status': u'NOT_FOUND', u'message': u'Field: name Error: The model resource: \"CNN_W2P_model_dnn\" was not found. Please create the Cloud ML model resource first by using \\'gcloud ml-engine models create CNN_W2P_model_dnn\\'.', u'code': 404, u'details': [{u'fieldViolations': [{u'field': u'name', u'description': u'The model resource: \"CNN_W2P_model_dnn\" was not found. Please create the Cloud ML model resource first by using \\'gcloud ml-engine models create CNN_W2P_model_dnn\\'.'}], u'@type': u'type.googleapis.com/google.rpc.BadRequest'}]}}\n",
      "{u'error': {u'status': u'NOT_FOUND', u'message': u'Field: name Error: The model resource: \"CNN_W2P_model_dnn\" was not found. Please create the Cloud ML model resource first by using \\'gcloud ml-engine models create CNN_W2P_model_dnn\\'.', u'code': 404, u'details': [{u'fieldViolations': [{u'field': u'name', u'description': u'The model resource: \"CNN_W2P_model_dnn\" was not found. Please create the Cloud ML model resource first by using \\'gcloud ml-engine models create CNN_W2P_model_dnn\\'.'}], u'@type': u'type.googleapis.com/google.rpc.BadRequest'}]}}\n",
      "{u'error': {u'status': u'NOT_FOUND', u'message': u'Field: name Error: The model resource: \"CNN_W2P_model_dnn\" was not found. Please create the Cloud ML model resource first by using \\'gcloud ml-engine models create CNN_W2P_model_dnn\\'.', u'code': 404, u'details': [{u'fieldViolations': [{u'field': u'name', u'description': u'The model resource: \"CNN_W2P_model_dnn\" was not found. Please create the Cloud ML model resource first by using \\'gcloud ml-engine models create CNN_W2P_model_dnn\\'.'}], u'@type': u'type.googleapis.com/google.rpc.BadRequest'}]}}\n",
      "{u'error': {u'status': u'NOT_FOUND', u'message': u'Field: name Error: The model resource: \"CNN_W2P_model_dnn\" was not found. Please create the Cloud ML model resource first by using \\'gcloud ml-engine models create CNN_W2P_model_dnn\\'.', u'code': 404, u'details': [{u'fieldViolations': [{u'field': u'name', u'description': u'The model resource: \"CNN_W2P_model_dnn\" was not found. Please create the Cloud ML model resource first by using \\'gcloud ml-engine models create CNN_W2P_model_dnn\\'.'}], u'@type': u'type.googleapis.com/google.rpc.BadRequest'}]}}\n",
      "{u'error': {u'status': u'NOT_FOUND', u'message': u'Field: name Error: The model resource: \"CNN_W2P_model_dnn\" was not found. Please create the Cloud ML model resource first by using \\'gcloud ml-engine models create CNN_W2P_model_dnn\\'.', u'code': 404, u'details': [{u'fieldViolations': [{u'field': u'name', u'description': u'The model resource: \"CNN_W2P_model_dnn\" was not found. Please create the Cloud ML model resource first by using \\'gcloud ml-engine models create CNN_W2P_model_dnn\\'.'}], u'@type': u'type.googleapis.com/google.rpc.BadRequest'}]}}\n",
      "{u'error': {u'status': u'NOT_FOUND', u'message': u'Field: name Error: The model resource: \"CNN_W2P_model_dnn\" was not found. Please create the Cloud ML model resource first by using \\'gcloud ml-engine models create CNN_W2P_model_dnn\\'.', u'code': 404, u'details': [{u'fieldViolations': [{u'field': u'name', u'description': u'The model resource: \"CNN_W2P_model_dnn\" was not found. Please create the Cloud ML model resource first by using \\'gcloud ml-engine models create CNN_W2P_model_dnn\\'.'}], u'@type': u'type.googleapis.com/google.rpc.BadRequest'}]}}\n",
      "{u'error': {u'status': u'NOT_FOUND', u'message': u'Field: name Error: The model resource: \"CNN_W2P_model_dnn\" was not found. Please create the Cloud ML model resource first by using \\'gcloud ml-engine models create CNN_W2P_model_dnn\\'.', u'code': 404, u'details': [{u'fieldViolations': [{u'field': u'name', u'description': u'The model resource: \"CNN_W2P_model_dnn\" was not found. Please create the Cloud ML model resource first by using \\'gcloud ml-engine models create CNN_W2P_model_dnn\\'.'}], u'@type': u'type.googleapis.com/google.rpc.BadRequest'}]}}\n",
      "{u'error': {u'status': u'NOT_FOUND', u'message': u'Field: name Error: The model resource: \"CNN_W2P_model_dnn\" was not found. Please create the Cloud ML model resource first by using \\'gcloud ml-engine models create CNN_W2P_model_dnn\\'.', u'code': 404, u'details': [{u'fieldViolations': [{u'field': u'name', u'description': u'The model resource: \"CNN_W2P_model_dnn\" was not found. Please create the Cloud ML model resource first by using \\'gcloud ml-engine models create CNN_W2P_model_dnn\\'.'}], u'@type': u'type.googleapis.com/google.rpc.BadRequest'}]}}\n"
     ]
    }
   ],
   "source": [
    "#result = []\n",
    "for y in range(1,10):#model_data.shape[0]):\n",
    "    raw_prices = model_data.iloc[y]\n",
    "    prices = [float(price) for price in raw_prices]\n",
    "    #print prices\n",
    "    data = {\n",
    "      'instances': [\n",
    "        {\n",
    "          'price': prices\n",
    "        },\n",
    "      ]\n",
    "    }\n",
    "    #print(data)\n",
    "    response = requests.post(api, json=data, headers=headers)\n",
    "    #print y/model_data.shape[0]\n",
    "    forecast = response.json()##['predictions'][0]['predicted'][0] ## (df5.price[(nn):(nn+J)])/60\n",
    "    print (forecast)\n",
    "    #result.append(response.content[32:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile  hyperparam.yaml\n",
    "trainingInput:\n",
    "  hyperparameters:\n",
    "    goal: MINIMIZE\n",
    "    maxTrials: 5\n",
    "    maxParallelTrials: 1\n",
    "    hyperparameterMetricTag: rmse\n",
    "    params:\n",
    "    - parameterName: learning_rate\n",
    "      type: DOUBLE\n",
    "      minValue: 0.01\n",
    "      maxValue: 0.1\n",
    "      scaleType: UNIT_LOG_SCALE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
